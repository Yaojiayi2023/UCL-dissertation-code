# -*- coding: utf-8 -*-
"""JiayiYao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJO3-UV-AZBW5K8aJlKuTqng0M2I7Vj7
"""

# 1.Link to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 1.Install bertopic
!pip install scipy<1.14.0
!pip install bertopic[flair,gensim,spacy,use]

# 1. Import necessary libraries
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 2. Load Excel data
path = '/content/drive/MyDrive/UCLFinal/ScammerInfoNovember2024.xlsx'
df = pd.read_excel(path)

# 3. View the first few lines
df.head()

# 1. Import the Pandas library and the pyplot module in Matplotlib
import pandas as pd
import matplotlib.pyplot as plt

# 2. Read data
df = pd.read_excel('/content/drive/MyDrive/UCLFinal/ScammerInfoNovember2024.xlsx')

# 3. Convert to datetime and extract the “beginning of the month” field
df['created_at'] = pd.to_datetime(df['created_at'])
# .dt.to_period('M').dt.to_timestamp() Default is the beginning of the month
df['month'] = df['created_at'].dt.to_period('M').dt.to_timestamp()

# 4. Count by month
monthly_counts = df.groupby('month').size()
print("Actual data month range:", monthly_counts.index.min(), "to", monthly_counts.index.max())

# 5. Generate a complete sequence from December 1, 2016 to the end of the month
all_months = pd.date_range(
    start="2016-12-01",
    end=monthly_counts.index.max(),
    freq='MS'      # MS = Month Start
)

# 6. Rebuild the index and fill in missing values with zeros
monthly_counts = monthly_counts.reindex(all_months, fill_value=0) \
                               .rename_axis('month') \
                               .reset_index(name='post_count')

print("Fill in the missing months:", monthly_counts['month'].min(), "to", monthly_counts['month'].max())
print(monthly_counts.head(5))

# 7. Drawing (including custom x-axis scale)
import matplotlib.dates as mdates

fig, ax = plt.subplots(figsize=(20, 10))
ax.plot(monthly_counts['month'], monthly_counts['post_count'], marker='x')

# 8. Set the x-axis: Display one tick mark every month (interval=1), format as year-month
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

plt.xlabel('Month')
plt.ylabel('Number of Posts')
plt.title('Number of Posts per Month (From Dec 2016)')
plt.grid(True, linestyle='--', alpha=0.5)

# 9. Rotate and align labels
plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
plt.tight_layout()
plt.show()

import pandas as pd
import re
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# 1) Read
df = pd.read_excel(
    "/content/drive/MyDrive/UCLFinal/ScammerInfoNovember2024.xlsx",
    parse_dates=["created_at"]
)

# 2) Remove _x000D_ and line breaks
for col in ["title", "excerpt", "tags"]:
    df[col] = df[col].fillna("").astype(str) \
        .str.replace("_x000D_", " ", regex=False) \
        .str.replace(r"[\r\n]+", " ", regex=True)

# 3) Text cleaning only
def clean_text(text: str) -> str:
    text = str(text).lower()
    text = re.sub(r"[^a-z0-9\s\+\-\(\)\@\.]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

for col in ["title", "excerpt"]:
    df[f"clean_{col}"] = df[col].apply(clean_text)

# 4) Filter empty text (discard only when both clean_title and clean_excerpt are empty)
mask_empty = (df["clean_title"].str.strip() == "") & (df["clean_excerpt"].str.strip() == "")
df = df[~mask_empty].reset_index(drop=True)

# 5) Topic modeling
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
topic_model = BERTopic(
    embedding_model=embedding_model,
    vectorizer_model=None,
    language="english"
)

docs = (df["clean_title"] + " " + df["clean_excerpt"]).str.strip().tolist()
topics, probs = topic_model.fit_transform(docs)
df["topic"] = topics
df["topic_prob"] = probs

# 6) Filtering low confidence and noise
threshold = 0.2
cleaned_df = df[(df["topic"] != -1) & (df["topic_prob"] >= threshold)].reset_index(drop=True)
cleaned_df["created_at"] = pd.to_datetime(cleaned_df["created_at"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")

# 7) Save
import shutil, time

cols_to_save = ["id", "created_at", "tags", "views", "clean_title", "clean_excerpt", "topic", "topic_prob"]

local_path = "/content/cleaned_posts.csv"
cleaned_df.to_csv(
    local_path,
    columns=cols_to_save,
    index=False,
    encoding="utf-8-sig"
)

# Read back and preview
check_df = pd.read_csv(local_path)
display(check_df.sample(5, random_state=42))

drive_path = "/content/drive/MyDrive/UCLFinal/cleaned_data_new.csv"
shutil.copyfile(local_path, drive_path)
time.sleep(1)

print(f"Success: {drive_path}")

# 1. Import necessary libraries
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# 2. Read Excel and process dates ——
path = "/content/drive/MyDrive/UCLFinal/cleaned_data.xlsx"
df = pd.read_excel(path)

# Convert the “created_at” column to “datetime”, then extract month
df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')
df['month'] = df['created_at'].dt.to_period('M').dt.to_timestamp()
df['month_str'] = df['month'].dt.strftime('%Y-%m')

# - email: name@domain.tld
# - phone: +?digits with -, space, ( )
# - word:  At least 2 characters (reduce a/i noise)
TOKEN_PATTERN = r"(?i)(?:[a-z0-9._%+\-]+@[a-z0-9.\-]+\.[a-z]{2,}|\+?\d[\d\-\s\(\)]{6,}\d|\b[a-z0-9]{2,}\b)"

# Expanded stop words
CUSTOM_STOP = {
    "number", "numbers",
    "extra", "info",
    "http", "https", "www",
    "scam", "scams", "scammer", "scammers",
    "website", "websites",
    "information", "additional",
    "email", "emails", "telephone"
}
STOPWORDS = list(ENGLISH_STOP_WORDS.union(CUSTOM_STOP))

# Precisely ignore multi-word phrases in the results stage（bigram/trigram）
IGNORE_EXACT_TERMS = {
    "extra info",
}
# ======================================

# 3. Define a function to obtain the Top-K n-grams.
def get_top_ngrams(docs, ngram_range=(1,1), top_k=5):
    """
    docs:      Text list
    ngram_range: (min_n, max_n)，(1,1)=unigram, (2,2)=bigram, (3,3)=trigram
    top_k:     Return the top k n-grams with the highest frequency
    """
    # Ensure all elements are strings and lowercase
    docs = [str(doc).lower() for doc in docs if isinstance(doc, str) or pd.notnull(doc)]

    vectorizer = CountVectorizer(
        ngram_range=ngram_range,
        stop_words=STOPWORDS,        # Use extended stop words
        token_pattern=TOKEN_PATTERN  # Capture email addresses, phone numbers, and common words at the same time.
    )
    X = vectorizer.fit_transform(docs)
    freqs = list(zip(vectorizer.get_feature_names_out(), X.sum(axis=0).A1))

    # Filter again for “exact phrases” at the result level.
    freqs = [(t, f) for (t, f) in freqs if t not in IGNORE_EXACT_TERMS]

    # Top-K
    return sorted(freqs, key=lambda x: x[1], reverse=True)[:top_k]

# 4. Group by “month” and calculate the top 5 unigrams/bigrams/trigrams for each month.

# Merge available text columns
text_columns = [c for c in ['title', 'excerpt', 'tags', 'emails', 'phones'] if c in df.columns]
if not text_columns:
    text_columns = ['excerpt']

results = []

for month_str, group in df.groupby('month_str'):
    # Prepare text: Put the required columns together
    docs = (group[text_columns]
            .fillna('')
            .astype(str)
            .agg(' '.join, axis=1)
            .tolist())

    # top 5 uni/bi/tri-grams
    top_uni = get_top_ngrams(docs, (1,1), top_k=5)
    top_bi  = get_top_ngrams(docs, (2,2), top_k=5)
    top_tri = get_top_ngrams(docs, (3,3), top_k=5)

    # summary
    for typ, grams in [('unigram', top_uni), ('bigram', top_bi), ('trigram', top_tri)]:
        for term, freq in grams:
            results.append({
                'month':       month_str,
                'ngram_type':  typ,
                'term':        term,
                'frequency':   int(freq)
            })

# 5. Convert to DataFrame and view
ngram_df = pd.DataFrame(results)

# Print examples by month and type
for m in sorted(ngram_df['month'].unique()):
    print(f"\n=== {m} ===")
    for typ in ['unigram', 'bigram', 'trigram']:
        sub = ngram_df[(ngram_df['month'] == m) & (ngram_df['ngram_type'] == typ)]
        print(f"  {typ}: ", list(zip(sub['term'], sub['frequency'])))

# Save as CSV
ngram_df.to_csv("monthly_ngrams.csv", index=False, encoding="utf-8-sig")
print("\nSaved results to   monthly_ngrams.csv")

import re
import pandas as pd
from urllib.parse import urlparse

# regular expression
EMAIL_RE  = re.compile(r"(?i)[A-Za-z0-9._%+\-]+@([A-Za-z0-9.\-]+\.[A-Za-z]{2,})")  # Capture only email domain names
PHONE_RE  = re.compile(r"(?i)\+?\d[\d\-\s\(\)]{6,}\d")
URL_RE    = re.compile(r"(?i)\bhttps?://[^\s/$.?#].[^\s]*")                        # URLs with http/https
DOMAIN_RE = re.compile(r"(?i)(?<!@)\b(?:[a-z0-9-]+\.)+[a-z]{2,}\b")               # Naked domain name (excluding email)

def normalize_phone(s: str) -> str:
    s = re.sub(r"[()\s\-]", "", s).replace("＋", "+")
    return re.sub(r"[^+\d]", "", s)

def phone_country_code(s: str) -> str:
    if not s.startswith("+"): return ""
    m = re.match(r"\+(\d{1,3})", s)  # 1st to 3rd place
    return f"+{m.group(1)}" if m else ""

def extract_domain(url: str) -> str:
    try:
        d = urlparse(url).netloc.lower()
        return d[4:] if d.startswith("www.") else d
    except Exception:
        return ""

# YYYY-MM
if 'month_str' not in df.columns:
    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')
    df['month_str']  = df['created_at'].dt.strftime('%Y-%m')

# # Prefer clean_* columns, then fall back to original columns
candidates = ['clean_title','clean_excerpt','title','excerpt','tags']
text_columns = [c for c in candidates if c in df.columns]
if not text_columns:
    raise ValueError("No usable text columns found. Please confirm that df contains at least one of clean_title/clean_excerpt or title/excerpt/tags.")
print("Using text columns:", text_columns)

email_rows, phonecc_rows, url_rows = [], [], []

for month_str, group in df.groupby('month_str'):
    docs_series = group[text_columns].fillna('').astype(str).agg(' '.join, axis=1)
    joined = " ".join(docs_series.tolist()).strip()
    if not joined:
        continue  # Skip if the text for the current month is empty.

    # Top 10 Email Domain Names
    email_domains = [d.lower() for d in EMAIL_RE.findall(joined)]
    if email_domains:
        for dom, cnt in pd.Series(email_domains, dtype="string").value_counts().head(10).items():
            email_rows.append({'month': month_str, 'term': dom, 'frequency': int(cnt)})

    # Top 10 Country Codes
    phones_norm = [normalize_phone(p) for p in PHONE_RE.findall(joined)]
    codes = [phone_country_code(p) for p in phones_norm if phone_country_code(p)]
    if codes:
        for cc, cnt in pd.Series(codes, dtype="string").value_counts().head(10).items():
            phonecc_rows.append({'month': month_str, 'term': cc, 'frequency': int(cnt)})

    # Top 10 Website Domains
    url_domains1 = [extract_domain(u) for u in URL_RE.findall(joined)]
    url_domains2 = DOMAIN_RE.findall(joined)  # Naked domain name (such as example.com)
    url_domains  = [d.lower() for d in url_domains1 + url_domains2 if d]
    if url_domains:
        for ud, cnt in pd.Series(url_domains, dtype="string").value_counts().head(10).items():
            url_rows.append({'month': month_str, 'term': ud, 'frequency': int(cnt)})

# Include columns even if they are empty to avoid errors when retrieving columns later
email_domain_df = pd.DataFrame(email_rows,  columns=['month','term','frequency'])
phone_cc_df     = pd.DataFrame(phonecc_rows, columns=['month','term','frequency'])
url_domain_df   = pd.DataFrame(url_rows,     columns=['month','term','frequency'])

# Print (print only months with data)
months = sorted(set(email_domain_df['month'].dropna().unique())
                | set(phone_cc_df['month'].dropna().unique())
                | set(url_domain_df['month'].dropna().unique()))

for m in months:
    print(f"\n=== {m} ===")
    sub_dom = email_domain_df[email_domain_df['month']==m]
    sub_cc  = phone_cc_df[phone_cc_df['month']==m]
    sub_url = url_domain_df[url_domain_df['month']==m]
    if not sub_dom.empty:
        print("  email_domain:", list(zip(sub_dom['term'], sub_dom['frequency'])))
    if not sub_cc.empty:
        print("  phone_cc:", list(zip(sub_cc['term'], sub_cc['frequency'])))
    if not sub_url.empty:
        print("  url_domain:", list(zip(sub_url['term'], sub_url['frequency'])))

# Save
email_domain_df.to_csv("monthly_email_domains.csv", index=False, encoding="utf-8-sig")
phone_cc_df.to_csv("monthly_phone_cc.csv",     index=False, encoding="utf-8-sig")
url_domain_df.to_csv("monthly_url_domains.csv",index=False, encoding="utf-8-sig")

print("\nSaved:\n  monthly_email_domains.csv\n  monthly_phone_cc.csv\n  monthly_url_domains.csv")

import re
import pandas as pd
from urllib.parse import urlparse

# Regular expressions
EMAIL_RE_FULL  = re.compile(r"(?i)[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}")
PHONE_RE_FULL  = re.compile(r"(?i)\+?\d[\d\-\s\(\)]{6,}\d")
URL_RE_FULL    = re.compile(r"(?i)\bhttps?://[^\s/$.?#].[^\s]*")                 # http/https URL
DOMAIN_RE_BARE = re.compile(r"(?i)(?<!@)\b(?:[a-z0-9-]+\.)+[a-z]{2,}\b")         # Naked domain name (excluding @ in email addresses)

def normalize_phone_full(s: str) -> str:
    """Remove spaces, parentheses, and hyphens, leaving only + and numbers."""
    s = re.sub(r"[()\s\-]", "", s).replace("＋", "+")
    return re.sub(r"[^+\d]", "", s)

def extract_domain(url: str) -> str:
    """Extract the domain name from the URL and remove www."""
    try:
        d = urlparse(url).netloc.lower()
        return d[4:] if d.startswith("www.") else d
    except Exception:
        return ""

# Text column (preferably clean_title / clean_excerpt)
text_columns = [c for c in ['clean_title', 'clean_excerpt'] if c in df.columns]
if not text_columns:
    raise ValueError("There is no clean_title / clean_excerpt in the data. Please confirm that the cleaning steps have been performed.")
print("Using text columns:", text_columns)

# Merge all texts
all_text = " ".join(df[text_columns].fillna('').astype(str).agg(' '.join, axis=1).tolist())

# Extract information
# Email
emails_full = [e.lower() for e in EMAIL_RE_FULL.findall(all_text)]
# Phone
phones_full = [normalize_phone_full(p) for p in PHONE_RE_FULL.findall(all_text)]
# URL + Domain
urls_full   = URL_RE_FULL.findall(all_text)
url_domains_from_urls = [extract_domain(u) for u in urls_full if extract_domain(u)]
bare_domains          = [d.lower() for d in DOMAIN_RE_BARE.findall(all_text)]
url_domains_all = [d for d in (url_domains_from_urls + bare_domains) if d]

# Convert to Series
emails_series      = pd.Series(emails_full, dtype="string")
phones_series      = pd.Series(phones_full, dtype="string")
url_domains_series = pd.Series(url_domains_all, dtype="string")

# Number
print(f"Number of unique email addresses: {emails_series.nunique(dropna=True)}")
print(f"Number of unique phone numbers: {phones_series.nunique(dropna=True)}")
print(f"Number of unique domains: {url_domains_series.nunique(dropna=True)}")

# Frequency of occurrence (Top 10)
email_counts  = emails_series.value_counts(dropna=True).reset_index()
email_counts.columns = ['email', 'frequency']

phone_counts  = phones_series.value_counts(dropna=True).reset_index()
phone_counts.columns = ['phone', 'frequency']

domain_counts = url_domains_series.value_counts(dropna=True).reset_index()
domain_counts.columns = ['domain', 'frequency']

print("\nTop 10 emails:")
print(email_counts.head(10))

print("\nTop 10 phone numbers:")
print(phone_counts.head(10))

print("\nTop 10 domains:")
print(domain_counts.head(10))

# Save
email_counts.to_csv("all_emails_frequency.csv", index=False, encoding="utf-8-sig")
phone_counts.to_csv("all_phones_frequency.csv", index=False, encoding="utf-8-sig")
domain_counts.to_csv("all_url_domains_frequency.csv", index=False, encoding="utf-8-sig")

print("\nSaved:")
print("  all_emails_frequency.csv")
print("  all_phones_frequency.csv")
print("  all_url_domains_frequency.csv")

# === Add-on A: imports & folders (non-destructive) ===
import os, re, json, pathlib, numpy as np, pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import umap, hdbscan
from sklearn.feature_extraction.text import CountVectorizer


BASE = "/content/drive/MyDrive/UCLFinal"
DATA_XLSX = f"{BASE}/ScammerInfoNovember2024.xlsx"
OUT_BASE  = f"{BASE}/bertopic_monthly"
CACHE_DIR = f"{BASE}/cache"

for p in [OUT_BASE, CACHE_DIR]:
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)

# === Add-on B: read & light clean (new dataframe) ===
from html import unescape
import re
# 1) Reading the table: Do not parse dates within `read_excel`; preserve the original values first.
df_bt = pd.read_excel(DATA_XLSX)

#(The cleaning of the remaining title/excerpt/tags remains unchanged.)
for col in ["title", "excerpt", "tags"]:
    if col in df_bt.columns:
        df_bt[col] = df_bt[col].fillna("").astype(str) \
            .str.replace("_x000D_", " ", regex=False) \
            .str.replace(r"[\r\n]+", " ", regex=True)

def _clean_text(x: str) -> str:
    s = unescape(str(x))               # Restore &lt; &gt; &amp; &quot;
    s = s.lower()

    #  URL / Email / Phone number
    s = re.sub(r'https?://\S+|www\.\S+', ' ', s)
    s = re.sub(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', ' ', s)
    s = re.sub(r'(?<!\w)(?:\+?\d[\d\-\s\(\)]{6,}\d)\b', ' ', s)

    # Only retain alphanumeric characters and spaces; compress whitespace.
    s = re.sub(r'[^a-z0-9\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()

    # Common Scam Phrases for Phrase Consolidation
    s = re.sub(r'\bgeek\s+squad\b', 'geek_squad', s)
    s = re.sub(r'\btech\s+support\b', 'tech_support', s)
    s = re.sub(r'\bmicrosoft\s+defender\b', 'microsoft_defender', s)
    s = re.sub(r'\bpublishers\s+clearing\s+house\b', 'publishers_clearing_house', s)
    s = re.sub(r'\bamazon\s+prime|prime\s*video\b', 'amazon_prime', s)
    s = re.sub(r'\bamazon\s+refund\b', 'amazon_refund', s)
    s = re.sub(r'\bnorton\s+refund\b', 'norton_refund', s)

    return s

for col in ["title", "excerpt"]:
    if col in df_bt.columns:
        df_bt[f"clean_{col}"] = df_bt[col].apply(_clean_text)

# 2) Combined Text
docs_bt = (df_bt.get("clean_title", df_bt["title"].fillna("")) + " " +
           df_bt.get("clean_excerpt", df_bt["excerpt"].fillna(""))).str.strip().tolist()

# 3) Robust Date Parsing: Multiple Candidate Columns + Excel Serial Number + dayfirst Attempt
import pandas as pd, re
DATE_CANDIDATES = ["created_at", "created", "date", "published_at", "post_date", "time"]

def _robust_parse_one(x):
    s = str(x).strip()
    if not s or s.lower() in {"nan", "nat", "none"}:
        return pd.NaT
    # Excel serial number
    if re.fullmatch(r"\d{5,6}", s):
        try:
            return pd.to_datetime(float(s), unit="d", origin="1899-12-30", utc=True)
        except Exception:
            pass
    # First try dayfirst=False, then try dayfirst=True.
    for kw in (dict(dayfirst=False), dict(dayfirst=True)):
        dt = pd.to_datetime(s, errors="coerce", utc=True, **kw)
        if not pd.isna(dt):
            return dt
    return pd.NaT

# Select the most reliable date column (the one with the lowest NaT ratio).
best_name, best_series, best_nat = None, None, 1.1
for c in DATE_CANDIDATES:
    if c not in df_bt.columns:
        continue
    ser = df_bt[c].apply(_robust_parse_one)
    nat_ratio = ser.isna().mean()
    if nat_ratio < best_nat:
        best_name, best_series, best_nat = c, ser, nat_ratio

if best_series is None:
    raise RuntimeError(f"None of these candidate columns exist.：{DATE_CANDIDATES}")

df_bt["created_at"] = best_series
df_bt["month_str"]  = df_bt["created_at"].dt.to_period("M").astype(str)

# 4) Check the monthly range and NaT ratio
valid_months = df_bt.loc[df_bt["created_at"].notna(), "month_str"]
print("Add-on data size:", len(docs_bt),
      "month range:", (valid_months.min() if not valid_months.empty else "N/A"),
      "→", (valid_months.max() if not valid_months.empty else "N/A"),
      "| NaT ratio:", round(df_bt["created_at"].isna().mean(), 4))

# === Add-on C: embeddings (cache) ===
EMB_PATH = f"{CACHE_DIR}/embeddings_en.npy"

if os.path.exists(EMB_PATH):
    try:
        emb_full = np.load(EMB_PATH)
        if emb_full.shape[0] != len(docs_bt):
            raise ValueError("cache size mismatch")
        print("Loaded cached embeddings:", emb_full.shape)
    except Exception as e:
        print("Rebuild embeddings due to:", e)
        sbert = SentenceTransformer("all-MiniLM-L6-v2")
        emb_full = sbert.encode(docs_bt, batch_size=64, show_progress_bar=True, normalize_embeddings=True)
        np.save(EMB_PATH, emb_full)
        print("Saved embeddings:", emb_full.shape)
else:
    sbert = SentenceTransformer("all-MiniLM-L6-v2")
    emb_full = sbert.encode(docs_bt, batch_size=64, show_progress_bar=True, normalize_embeddings=True)
    np.save(EMB_PATH, emb_full)
    print("Saved embeddings:", emb_full.shape)

# === Add-on E: printing/saving topic representations ===
def print_and_save_topic_reprs(topic_model: BERTopic, out_dir: str, top_n: int = 10) -> str:
    info = topic_model.get_topic_info()
    rows = []
    for _, r in info[info.Topic != -1].iterrows():
        tid, size = int(r.Topic), int(r.Count)
        words = [w for w, _ in topic_model.get_topic(tid)[:top_n]]
        print(f"Topic {tid} (size={size}): " + ", ".join(words))
        rows.append({"topic_id": tid, "size": size, "top_words": ", ".join(words)})
    out_csv = os.path.join(out_dir, "topic_representations.csv")
    pd.DataFrame(rows).to_csv(out_csv, index=False, encoding="utf-8-sig")
    return out_csv

# === Add-on D_superrobust: builder tuned for ~10–20 topics ===
import umap, hdbscan
from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as ENG_SW

CUSTOM_STOP = {"s","t","amp","utm","nbsp",
    "indexhtml","presentphp","primepreloaderhtml","php","html","preloader","loader",
    "used","domain","domains","registrant",
    "pm","est","uid","hxxp","number", "numbers",
    "extra", "info",
    "http", "https", "www",
    "scam", "scams", "scammer", "scammers",
    "website", "websites",
    "information", "additional",
    "email", "emails", "telephone",
    "screen","reader","readers","accessible","accessibility",
    "content","important","write","sure",
    # Webpage/Script Path & Marketing Parameters
    "hellip","indexphp","winxfiles","werrx01usahtml","merrx01usahtml","bemobdata",
    "gclid","aclk","clickid","subid",
    "lt","gt","amp","quot","nbsp",
    "posted","post","link","view","click",
    "please","dear","sir","madam","thanks","thank","regards","team",
    "today","tomorrow","yesterday","issue","problem",
    "http","https","www","com","org","net"             }

# Preprocessing: Remove URLs/email addresses/phone numbers/pure numbers/extremely long random strings
def vec_preproc(s: str) -> str:
    if not isinstance(s, str): return ""
    s = s.lower()
    s = re.sub(r'https?://\S+|www\.\S+', ' ', s)             # URL
    s = re.sub(r'\b[\w\.-]+@[\w\.-]+\.\w{2,}\b', ' ', s)      # email
    s = re.sub(r'(?<!\w)(?:\+?\d[\d\-\s]{6,}\d)\b', ' ', s)   # phone
    s = re.sub(r'\b\d{2,}\b', ' ', s)                         # numbers
    s = re.sub(r'\b[a-z]*\d+[a-z\d]*\b', ' ', s)              # Alphanumeric Mixed-Format Words
    s = re.sub(r'\b[0-9a-z]{20,}\b', ' ', s)                  # Extremely long suspected random string
    s = re.sub(r'\s+', ' ', s).strip()
    return s


def build_topic_model_for_n(n_docs: int):
    mcs = max(5, int(0.03 * n_docs))      # ← CHANGED: Original ~0.12*n → ~0.04*n (Smaller clusters → More themes)
    ms  = max(1, int(0.01 * n_docs))      # ← CHANGED: Original ~0.06*n → ~0.01*n (reduced outliers)

    umap_model = umap.UMAP(
        n_neighbors=12,                    # ← CHANGED: Original 30 → 12 (Focus more on the details)
        n_components=25,                   # ← CHANGED: Original 15 → 25 (preserving more structure)
        min_dist=0.00,                     # ← CHANGED: Original 0.10 → 0.00 (Cluster tighter)
        metric="cosine", random_state=42
    )
    hdbscan_model = hdbscan.HDBSCAN(
        min_cluster_size=mcs, min_samples=ms,
        cluster_selection_method="leaf",    # ← CHANGED: Original “eom” → “leaf” (more clusters)
        cluster_selection_epsilon=0.0,      # ← CHANGED: Original 0.25 → 0.0 (fewer merges)
        prediction_data=True
    )

    min_ratio = max(1 / max(n_docs, 1), 0.0005)  # ← ← CHANGED: Original max(2/n, 0.001) → Wider threshold (≥1 article or ≥0.05%)
    max_ratio = 0.95                  # ← CHANGED: Original 0.85 → 0.95 (reduced high-frequency word trimming)
    if min_ratio >= max_ratio:

        min_ratio, max_ratio = 0.0, 1.0
        ngram = (1, 1)
    else:
        ngram = (1, 3)                            # ← CHANGED: Original (1,2) → (1,3) (Phrase better distinguishes subject)

    vectorizer_model = CountVectorizer(
    stop_words=list(set(ENG_SW) | CUSTOM_STOP),   # ← Merge stopwords
    ngram_range=ngram,
    min_df=min_ratio,
    max_df=max_ratio,
    # The token must contain at least one letter: automatically excludes pure numeric strings and most encodings.
    token_pattern=r"(?u)\b(?=[a-z0-9_]*[a-z])[a-z0-9_]{2,}\b",
    preprocessor=vec_preproc
)


    # Aim to consolidate monthly themes to ~15 (within the 10–20 range)
    TARGET_TOPICS = 15

    tm = BERTopic(
        embedding_model=None,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,
        language="english",
        top_n_words=10,
        nr_topics=TARGET_TOPICS,                  # ← Merge permanently to ~15
        calculate_probabilities=True,
        verbose=False
    )
    params = {
        "n_docs": n_docs,
        "umap": {"n_neighbors":12,"n_components":25,"min_dist":0.00,"metric":"cosine"},
        "hdbscan": {"min_cluster_size":mcs,"min_samples":ms,"epsilon":0.0,"method":"leaf"},
        "vectorizer": {"ngram_range":str(ngram),"min_df":float(min_ratio),"max_df":float(max_ratio)},
        "nr_topics": TARGET_TOPICS               # ← CHANGED: Consistent with the above
    }
    return tm, params

print("builder tuned for ~10–20 topics (target ≈15).")

from math import floor, ceil
bad = []
for m, g in df_bt.groupby("month_str"):
    n = len(g)
    min_doc = max(2, int(0.02 * n))          # min_df
    max_doc = floor(0.85 * n)                # max_df
    if (n < 5) or (max_doc < min_doc):
        bad.append((m, n, min_doc, max_doc))
pd.DataFrame(bad, columns=["month","n","min_doc","max_doc"]).sort_values("n")

# Add before the monthly cycle; load the model only once.
from sentence_transformers import SentenceTransformer

if "sbert_monthly" not in globals():
    sbert_monthly = SentenceTransformer("all-MiniLM-L6-v2")

def embed_texts(texts):
    # Monthly volumes typically range from several hundred to one or two thousand entries, encoded directly on-site to avoid alignment issues with the emb_full slice.
    return sbert_monthly.encode(
        texts, batch_size=128, show_progress_bar=False, normalize_embeddings=True
    )

# === E_embed_helper (minimal) ===

if "embed_texts" not in globals():
    try:
        from sentence_transformers import SentenceTransformer
    except ModuleNotFoundError:

        raise
    # Load the model only once (reuse it for subsequent months to avoid repeated monthly loading)
    if "sbert_monthly" not in globals():
        sbert_monthly = SentenceTransformer("all-MiniLM-L6-v2")
    def embed_texts(texts, batch_size=128):
        return sbert_monthly.encode(
            texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True
        )
    print("embed_texts() is ready:", sbert_monthly.__class__.__name__)
else:
    print("embed_texts() already defined; reusing existing model.")

# === Add-on F_robust_noalign: per-month, no slicing of emb_full ===
from pathlib import Path
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

MIN_DOCS_TO_RUN = 2
summary = []

def make_fallback_vec():
    # Most lenient: No stopword filtering; unigram; completely unrestricted; document frequency threshold set to proportion
    return CountVectorizer(
        stop_words=list(set(ENG_SW) | CUSTOM_STOP),
        ngram_range=(1, 1),
        min_df=0.0, max_df=1.0,
        token_pattern=r"(?u)\b[a-z0-9_]+\b"
    )

def prefit_and_mask(vec, texts):
    """Return (fitted vec, mask(whether each article has ≥1 token), n_features). Throw ValueError on failure."""
    X = vec.fit_transform(texts)
    mask = (np.asarray(X.sum(axis=1)).ravel() > 0)
    if X.shape[1] == 0 or mask.sum() == 0:
        raise ValueError("after pruning, no terms remain")
    return vec, mask, X.shape[1]

def need_vec_fallback(msg: str) -> bool:
    msg = msg.lower()
    return ("max_df corresponds to" in msg) or ("empty vocabulary" in msg) or ("after pruning" in msg)

for month_str, g in df_bt.groupby("month_str"):
    # 1) Remove whitespace (index-text alignment)
    idx_all  = g.index.to_numpy()
    docs_all = [docs_bt[i] for i in idx_all]
    pairs = [(i, d) for i, d in zip(idx_all, docs_all) if isinstance(d, str) and d.strip()]

    if len(pairs) < MIN_DOCS_TO_RUN:
        print(f"[{month_str}] non-empty docs={len(pairs)} < {MIN_DOCS_TO_RUN} → skip")
        continue

    idx      = np.array([i for i, _ in pairs], dtype=int)
    sub_docs = [d for _, d in pairs]
    n_docs   = len(sub_docs)

    # 2) Build a “larger/fewer themes” model (recommended using the proportional version of the builder from D_superrobust)
    tm, used_params = build_topic_model_for_n(n_docs)

    # 3) Pre-check vectorizer: If it fails at this stage, fall back to the most lenient option; always proceed to subsequent steps based on “text that passed pre-checking.”
    tried_fallback = False
    while True:
        try:
            v_fit, mask, nfeat = prefit_and_mask(tm.vectorizer_model, sub_docs)
            break
        except ValueError as e:
            if need_vec_fallback(str(e)) and not tried_fallback:
                tm.vectorizer_model = make_fallback_vec()
                tried_fallback = True
                continue
            else:
                print(f"[{month_str}] prefit failed → skip ({e})")
                mask = None
                break

    if mask is None or mask.sum() < MIN_DOCS_TO_RUN:
        print(f"[{month_str}] docs with tokens={0 if mask is None else int(mask.sum())} < {MIN_DOCS_TO_RUN} → skip")
        continue

    # 4) Filter text using a mask (do not cut emb_full; we will recalculate embeddings directly later)
    sub_idx  = idx[mask]
    sub_docs = [d for d, m in zip(sub_docs, mask) if m]

    # 5) On-site recalculation of the SBERT vector for that month (completely avoiding alignment issues like 0×384)
    sub_emb = embed_texts(sub_docs)
    if len(sub_emb) == 0:
        print(f"[{month_str}] embedding produced 0 samples → skip")
        continue

    # 6) Training: If pruning/thresholding errors persist, revert once more and re-extract text + recalculate embeddings based on the new mask generated from the reverted state.
    try:
        topics, probs = tm.fit_transform(sub_docs, embeddings=sub_emb)
    except ValueError as e:
        if need_vec_fallback(str(e)):
            v2 = make_fallback_vec()
            try:
                v2, mask2, nfeat2 = prefit_and_mask(v2, sub_docs)
            except ValueError as e2:
                print(f"[{month_str}] training fallback failed → skip ({e2})")
                continue
            sub_docs2 = [d for d, m in zip(sub_docs, mask2) if m]
            if len(sub_docs2) < MIN_DOCS_TO_RUN:
                print(f"[{month_str}] fallback docs with tokens={len(sub_docs2)} < {MIN_DOCS_TO_RUN} → skip")
                continue
            sub_emb2 = embed_texts(sub_docs2)  #  After rolling back, recalculate the embeddings once more.
            tm.vectorizer_model = v2
            topics, probs = tm.fit_transform(sub_docs2, embeddings=sub_emb2)
            sub_docs, sub_emb, sub_idx = sub_docs2, sub_emb2, sub_idx[mask2]
        else:
            print(f"[{month_str}] unexpected fit error → skip ({e})")
            continue

   # 7) Output to the folder for that month
    out_dir = os.path.join(OUT_BASE, month_str)
    Path(out_dir).mkdir(parents=True, exist_ok=True)

    info_df = tm.get_topic_info()
    info_df.to_csv(os.path.join(out_dir, "topic_info.csv"), index=False, encoding="utf-8-sig")

    doc_df = tm.get_document_info(sub_docs)
    doc_df["orig_index"] = sub_idx
    if "id" in g.columns:
        doc_df["id"] = g.loc[sub_idx, "id"].values
    doc_df["created_at"] = g.loc[sub_idx, "created_at"].values
    doc_df.to_csv(os.path.join(out_dir, "document_topics.csv"), index=False, encoding="utf-8-sig")

    # 8) Print and save representative words
    rows = []
    for _, r in info_df[info_df.Topic != -1].iterrows():
        tid, size = int(r.Topic), int(r.Count)
        words = [w for w, _ in tm.get_topic(tid)[:10]]
        print(f"[{month_str}] Topic {tid} (size={size}): " + ", ".join(words))
        rows.append({"topic_id": tid, "size": size, "top_words": ", ".join(words)})
    pd.DataFrame(rows).to_csv(os.path.join(out_dir, "topic_representations.csv"),
                              index=False, encoding="utf-8-sig")

    with open(os.path.join(out_dir, "params.json"), "w", encoding="utf-8") as f:
        json.dump(used_params, f, ensure_ascii=False, indent=2)

    n_topics = int((info_df["Topic"] != -1).sum())
    summary.append({"month": month_str, "n_docs": len(sub_docs), "n_topics": n_topics})
    print(f"[{month_str}] done: docs={len(sub_docs)}, topics(≠-1)={n_topics}; out → {out_dir}")

# 9) Summary
summary_path = os.path.join(OUT_BASE, "monthly_summary.csv")
pd.DataFrame(summary).sort_values("month").to_csv(summary_path, index=False, encoding="utf-8-sig")
print("Saved summary:", summary_path)

# === Add-on G_fixed: print topic representations from any available source ===
import os, re, pandas as pd
from bertopic import BERTopic

TOP_N = 10       # How many words before each topic is displayed
MAX_TOPICS = 50  # Maximum number of topics that can be printed to prevent flooding the screen

def print_model_topics(model: BERTopic, label="[MODEL]"):
    info = model.get_topic_info()
    n = int((info["Topic"] != -1).sum())
    print(f"{label} topics(≠-1) = {n}")
    shown = 0
    for _, r in info[info.Topic != -1].iterrows():
        tid = int(r.Topic); size = int(r.Count)
        words = [w for w, _ in model.get_topic(tid)[:TOP_N]]
        print(f"{label} Topic {tid} (size={size}): " + ", ".join(words))
        shown += 1
        if shown >= MAX_TOPICS:
            print(f"... (only first {MAX_TOPICS} topics shown)")
            break

printed = False

# 1) Attempt to locate the “trained” BERTopic instance in memory
candidates = []
for name, val in list(globals().items()):
    if isinstance(val, BERTopic):
        try:
            _ = val.get_topic_info()  # If not trained, it will throw an error.
            candidates.append((name, val))
        except Exception:
            pass

# If there are multiple options, prioritize those whose names contain ‘all’ or ‘topic_model’.
if candidates:
    candidates.sort(key=lambda nv: (not any(k in nv[0].lower() for k in ["all","topic_model"]), nv[0]))
    name, model = candidates[0]
    print_model_topics(model, label=f"[{name}]")
    printed = True

# 2) If no trained model exists in memory, print from the monthly CSV (the results you saved earlier).
if not printed:

    out_base = OUT_BASE if "OUT_BASE" in globals() else "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
    if os.path.isdir(out_base):
        # Find all month folders named in the format YYYY-MM
        months = sorted([m for m in os.listdir(out_base) if re.fullmatch(r"\d{4}-\d{2}", m)])
        if not months:
            print("No trained model in memory and no monthly folders found.")
        else:
           # Select the most recent one or several print jobs
            for m in months[:3]:
                csv_path = os.path.join(out_base, m, "topic_representations.csv")
                if os.path.exists(csv_path):
                    df = pd.read_csv(csv_path)
                    print(f"[{m}] topics = {len(df)}")
                    for _, row in df.iterrows():
                        print(f"[{m}] Topic {int(row['topic_id'])} (size={int(row['size'])}): {row['top_words']}")
                    printed = True
                else:
                    print(f"[{m}] missing topic_representations.csv")
    else:
        print("OUT_BASE not found and no in-memory model available.")

if not printed:
    has_tm = "topic_model" in globals()
    print(f"(Nothing printed: trained BERTopic not found in memory; OUT_BASE scanned={'yes' if os.path.isdir(out_base) else 'no'})")

# Visualization Export Settings
MIN_DOCS_TO_RUN = 2   # At least 2 documents to be re-modeled

MAKE_HTML  = True
MAKE_PNG   = False
IMG_WIDTH  = 1400
IMG_HEIGHT = 900
IMG_SCALE  = 2
MAX_DOCS_SCATTER = 2500

import os, pathlib
import numpy as np

if "embed_texts" not in globals():
    from sentence_transformers import SentenceTransformer
    if "sbert_monthly" not in globals():
        sbert_monthly = SentenceTransformer("all-MiniLM-L6-v2")
    def embed_texts(texts, batch_size=128):
        return sbert_monthly.encode(
            texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True
        )

def _save_fig(fig, out_dir, name):
    path_html = os.path.join(out_dir, f"{name}.html")
    path_png  = os.path.join(out_dir, f"{name}.png")
    if MAKE_HTML:
        fig.write_html(path_html)
    if MAKE_PNG:
        fig.write_image(path_png, width=IMG_WIDTH, height=IMG_HEIGHT, scale=IMG_SCALE)

def _maybe(fig_func, *args, out_dir=None, name=None, **kwargs):
    try:
        fig = fig_func(*args, **kwargs)
        _save_fig(fig, out_dir, name)
        return True
    except Exception as e:
        print(f"[skip {name}] {e}")
        return False

for month_str, g in df_bt.groupby("month_str"):
    if pd.isna(month_str):
        print("[skip NaT month]"); continue

    # Subset
    idx = g.index.to_numpy()
    sub_docs = [docs_bt[i] for i in idx]
    n_docs   = len(sub_docs)
    if n_docs == 0:
        print(f"[{month_str}] no docs → skip"); continue

    # Skip the small-month data to avoid UMAP single-sample errors.
    if n_docs < MIN_DOCS_TO_RUN:
        print(f"[{month_str}] docs={n_docs} < {MIN_DOCS_TO_RUN} → skip")
        continue

    # Output Directory
    out_dir = os.path.join(OUT_BASE, month_str)
    pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)

    # 1) Obtain embeddings: Use emb_full if available; otherwise, encode on-the-fly and cache.
    if "emb_full" in globals() and getattr(emb_full, "shape", (0,))[0] == len(docs_bt):
        sub_emb = emb_full[idx]
    else:
        cache_emb = os.path.join(out_dir, "embeddings.npy")
        if os.path.exists(cache_emb):
            sub_emb = np.load(cache_emb)
        else:
            sub_emb = embed_texts(sub_docs)   # No longer dependent on emb_full
            np.save(cache_emb, sub_emb)

        # Ensure the cache matches the number of documents for the current month; recalculate if there is a discrepancy.
        if getattr(sub_emb, "shape", (0,))[0] != n_docs:
            sub_emb = embed_texts(sub_docs)
            np.save(cache_emb, sub_emb)

    # 2) Modeling
    tm, _ = build_topic_model_for_n(n_docs)
    try:
        _topics, _probs = tm.fit_transform(sub_docs, embeddings=sub_emb)
    except ValueError as e:
        if "single data sample" in str(e):
            print(f"[{month_str}] only one sample after preprocessing → skip")
            continue
        else:
            raise

    # 3) Count non-outlier topics
    info_df  = tm.get_topic_info()
    n_topics = int((info_df["Topic"] != -1).sum())
    print(f"[{month_str}] topics(≠-1) = {n_topics}")

    # 4) Export Visualization
    _maybe(tm.visualize_hierarchy, out_dir=out_dir, name="hierarchy")
    if n_topics >= 2:
        _maybe(tm.visualize_heatmap, top_n_topics=min(20, n_topics), out_dir=out_dir, name="heatmap")

    # Document 2D Scatter Plot (Sample if too large)
    if len(sub_docs) > MAX_DOCS_SCATTER:
        sel = np.random.choice(len(sub_docs), MAX_DOCS_SCATTER, replace=False)
        docs_vis = [sub_docs[i] for i in sel]
        emb_vis  = sub_emb[sel]
    else:
        docs_vis = sub_docs
        emb_vis  = sub_emb

    _maybe(tm.visualize_documents, docs_vis, embeddings=emb_vis, out_dir=out_dir, name="documents")
    _maybe(tm.visualize_barchart, top_n_topics=min(20, max(1, n_topics)), out_dir=out_dir, name="barchart")
    _maybe(tm.visualize_topics, out_dir=out_dir, name="topics")

print("Monthly images have been generated and saved in their respective monthly folders")

RUN_ALL_IMAGES = True
if RUN_ALL_IMAGES:
    import os, pathlib, numpy as np

    # Fallback: Ensure embed_texts is available
    if "embed_texts" not in globals():
        from sentence_transformers import SentenceTransformer
        if "sbert_monthly" not in globals():
            sbert_monthly = SentenceTransformer("all-MiniLM-L6-v2")
        def embed_texts(texts, batch_size=128):
            return sbert_monthly.encode(
                texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True
            )

    # Use your entire text database (consistent with the preceding content)
    docs_all = docs_bt

    OUT_ALL = os.path.join(OUT_BASE, "_ALL_")
    pathlib.Path(OUT_ALL).mkdir(parents=True, exist_ok=True)

    # Unified full embeddings (reuse emb_full first, read from cache second, otherwise encode on-the-fly and cache)
    if "emb_full" in globals() and getattr(emb_full, "shape", (0,))[0] == len(docs_all):
        emb_all = emb_full
    else:
        cache_all = os.path.join(OUT_ALL, "embeddings.npy")
        if os.path.exists(cache_all):
            emb_all = np.load(cache_all)
        else:
            emb_all = embed_texts(docs_all)
            np.save(cache_all, emb_all)

    if "topic_model" in globals():
        tm_all = topic_model
        try:
            _ = tm_all.get_topic_info()
        except Exception:
            tm_all, _ = build_topic_model_for_n(len(docs_all))
            tm_all.fit_transform(docs_all, embeddings=emb_all)
    else:
        tm_all, _ = build_topic_model_for_n(len(docs_all))
        tm_all.fit_transform(docs_all, embeddings=emb_all)

        # Export images (Note: use `emb_all` for `visualize_documents`)
    _save_fig(tm_all.visualize_hierarchy(), OUT_ALL, "hierarchy")
    info_all  = tm_all.get_topic_info()
    n_all     = int((info_all["Topic"] != -1).sum())
    if n_all >= 2:
        _save_fig(tm_all.visualize_heatmap(top_n_topics=min(20, n_all)), OUT_ALL, "heatmap")
    _save_fig(tm_all.visualize_documents(docs_all, embeddings=emb_all), OUT_ALL, "documents")
    _save_fig(tm_all.visualize_barchart(top_n_topics=min(20, max(1, n_all))), OUT_ALL, "barchart")
    _save_fig(tm_all.visualize_topics(), OUT_ALL, "topics")

    print("out：", OUT_ALL)

# -- Fix: universal embed_texts wrapper with show_progress --
try:
    model_for_embed = sbert
except NameError:
    try:
        model_for_embed = sbert_monthly
    except NameError:
        from sentence_transformers import SentenceTransformer
        model_for_embed = SentenceTransformer("all-mpnet-base-v2")

def embed_texts(texts, batch_size=128, show_progress=False):
    return model_for_embed.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=bool(show_progress),
        normalize_embeddings=True
    )

# === Add-on H+: New Post “Have You Seen This Topic?” Online Matching ===
# ==== Cell 1: setup & helpers ====
import os, re, json
import numpy as np
import pandas as pd

# 1) Sentence vectors: Prioritize MPNet for higher accuracy; fall back to MiniLM if unavailable.
try:
    from sentence_transformers import SentenceTransformer
    _MODEL_NAME = "all-mpnet-base-v2"
    sbert = SentenceTransformer(_MODEL_NAME)
except Exception:
    from sentence_transformers import SentenceTransformer
    _MODEL_NAME = "all-MiniLM-L6-v2"
    sbert = SentenceTransformer(_MODEL_NAME)

def embed_texts(texts, batch_size=128, show_progress=False):
    return sbert.encode(texts, batch_size=batch_size, show_progress_bar=show_progress, normalize_embeddings=True)

# 2) Month Sort Key
def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

# 3) Generate more stable labels from top_words
def label_from_top_words(s: str, k=6):
    if not isinstance(s, str):
        return ""
    toks = [w.strip() for w in s.split(",") if w.strip()]
    DROP = {"hellip","amp","nbsp","quot"}
    out, seen = [], set()
    for w in toks:
        wl = w.lower()
        if wl in DROP:
            continue
        if w not in seen:
            out.append(w); seen.add(w)
        if len(out) >= k:
            break
    return " ; ".join(out) if out else ""

# 4) Text Light Cleaning (for generating a “streamlined query” variant)
STOP = set("""a an the and or to for of in on with from at as by is are be was were been
this that these those your their our you we they he she it not no do does did can could
should would may might will just if but so than then when where who what which how why
""".split())

def clean_text_min(s: str) -> str:
    s = s.lower()
    s = re.sub(r'https?://\S+|www\.\S+', ' ', s)
    s = re.sub(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', ' ', s)
    s = re.sub(r'(?<!\w)(?:\+?\d[\d\-\s]{6,}\d)\b', ' ', s)
    s = re.sub(r'\b\d+\b', ' ', s)
    toks = re.findall(r"[a-z][a-z0-9]+", s)
    toks = [t for t in toks if t not in STOP]
    return " ".join(toks)

# 5) Coarse-grained brand extraction (for candidate filtering)
BRANDS = [
    "microsoft","windows","defender","security","paypal","amazon","prime","geek squad","norton","mcafee",
    "pch","publishers clearing house","apple","iphone","facebook","coinbase","medicare","warranty","loan",
]
BRAND_PAT = re.compile("|".join(sorted(map(re.escape, BRANDS), key=len, reverse=True)), re.I)

def extract_brands(s: str):
    return sorted(set(m.group(0).lower() for m in BRAND_PAT.finditer(s or "")))

# ==== Cell 2: build global theme index from monthly outputs ====

OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"


MAX_DOCS_PER_TOPIC = 5       # For each “month-theme,” select several representative documents to determine the center of mass.
GROUP_THRESHOLD    = 0.78    # Threshold for merging cross-month data into a single “global theme” (adjustable between 0.76 and 0.82)

# 1) First collect all “Month-Theme” entries
topic_entries = []  # dict(month, topic_id, size, label, emb_label, emb_docs, brands)

month_dirs = sorted(
    [d for d in os.listdir(OUT_BASE) if re.fullmatch(r"\d{4}-\d{2}", d)],
    key=month_key
)

for m in month_dirs:
    rep_csv = os.path.join(OUT_BASE, m, "topic_representations.csv")
    doc_csv = os.path.join(OUT_BASE, m, "document_topics.csv")
    if not os.path.exists(rep_csv):
        continue
    rep = pd.read_csv(rep_csv)

    # Compatible Column Names
    if "top_words" not in rep.columns:
        for col in ["Words","Representation"]:
            if col in rep.columns:
                rep = rep.rename(columns={col:"top_words"})
                break
    if "size" not in rep.columns and "Count" in rep.columns:
        rep = rep.rename(columns={"Count":"size"})
    if "topic_id" not in rep.columns and "Topic" in rep.columns:
        rep = rep.rename(columns={"Topic":"topic_id"})

    # Filter outliers
    rep = rep[rep["topic_id"] != -1].copy()
    if rep.empty:
        continue

    # Document Table
    docs = None
    if os.path.exists(doc_csv):
        docs = pd.read_csv(doc_csv)
        # Compatible Column List
        if "Topic" in docs.columns and "topic_id" not in docs.columns:
            docs = docs.rename(columns={"Topic":"topic_id"})
        if "Document" not in docs.columns:
            # Some versions list the name as ‘Document’; if not, it's fine (just use the tag for embedding).
            docs = None

    # Construct two embeddings for each “month-theme”
    rep["label"] = rep["top_words"].apply(label_from_top_words)
    labels = rep["label"].fillna("").tolist()
    emb_labels = embed_texts(labels, show_progress=False)

    for i, row in rep.reset_index(drop=True).iterrows():
        tid   = int(row["topic_id"])
        size  = int(row.get("size", 0))
        label = row["label"] or ""
        emb_l = emb_labels[i]


        emb_d = None
        if docs is not None:
            dsub = docs[docs["topic_id"] == tid]
            if "Probability" in dsub.columns:
                dsub = dsub.sort_values("Probability", ascending=False)
            texts = dsub["Document"].astype(str).head(MAX_DOCS_PER_TOPIC).tolist()
            if texts:
                emb_d = embed_texts(texts, show_progress=False).mean(axis=0)
                emb_d = emb_d / (np.linalg.norm(emb_d) + 1e-9)

        topic_entries.append({
            "month": m, "topic_id": tid, "size": size,
            "label": label,
            "emb_label": emb_l.astype(np.float32),
            "emb_docs":  (emb_d.astype(np.float32) if emb_d is not None else None),
            "brands": extract_brands(label)
        })

print(f"Collected monthly topics: {len(topic_entries)} from {len(month_dirs)} months. Model = {_MODEL_NAME}")

# 2) Cross-month merges into the “global theme repository” (single-pass incremental aggregation)
themes = []  #dict(id, centroid, months(set), examples(list of indices), first_month, brands(set))
next_id = 0

def add_theme(entry_idx):
    global next_id
    e = topic_entries[entry_idx]
    # Use “tag embedding” as the starting point for the thematic center of gravity (more stable); document centers of gravity will be incorporated later.
    c = e["emb_label"].copy()
    themes.append({
        "id": next_id,
        "centroid": c,
        "months": {e["month"]},
        "examples": [entry_idx],
        "first_month": e["month"],
        "brands": set(e["brands"])
    })
    next_id += 1
    return themes[-1]

def theme_sim(centroid, e):
    # Maximum similarity between the centroid and the candidate (label or document centroid)
    sims = [float(np.dot(centroid, e["emb_label"]))]
    if e["emb_docs"] is not None:
        sims.append(float(np.dot(centroid, e["emb_docs"])))
    return max(sims)

def update_centroid(theme, e):
    # Update center of mass using “weighted average”: Label embeddings + (if available) document center of mass (with slightly higher weight toward documents)
    cur = theme["centroid"]
    add = e["emb_label"]
    if e["emb_docs"] is not None:
        add = 0.5*e["emb_label"] + 0.5*e["emb_docs"]
        add = add / (np.linalg.norm(add) + 1e-9)
    new = (cur + add) / (np.linalg.norm(cur + add) + 1e-9)
    theme["centroid"] = new

# Iterate through each month in chronological order
order = sorted(range(len(topic_entries)),
               key=lambda i: month_key(topic_entries[i]["month"]))
for idx in order:
    e = topic_entries[idx]
    if not themes:
        add_theme(idx);
        continue
    cents = np.vstack([t["centroid"] for t in themes])
    cand_sim = np.dot(cents, e["emb_label"])
    if e["emb_docs"] is not None:
        cand_sim = np.maximum(cand_sim, np.dot(cents, e["emb_docs"]))
    j = int(np.argmax(cand_sim)); best = float(cand_sim[j])
    if best >= GROUP_THRESHOLD:
        t = themes[j]
        t["months"].add(e["month"])
        t["examples"].append(idx)
        t["brands"] |= set(e["brands"])
        update_centroid(t, e)
    else:
        add_theme(idx)

# 3) Summary (Initial Month + Recurrence Month)
for t in themes:
    t["months"] = sorted(list(t["months"]), key=month_key)
    t["first_month"] = t["months"][0]

# 4) Packaged into an index object
theme_index = {
    "embedder": _MODEL_NAME,
    "group_threshold": GROUP_THRESHOLD,
    "themes": themes,
    "topic_entries": topic_entries  # Traceable to the original “Moon-Theme”
}

print(f"Built global themes: {len(themes)}")
print("Example theme:",
      {"id": themes[0]["id"], "first_month": themes[0]["first_month"],
       "months": themes[0]["months"][:5], "brands": sorted(list(themes[0]["brands"]))})

# ==== Cell 3 match a new post ====
def _make_queries(text: str):
    """Generate 3 query views for the same post: Original / Streamlined / Brand Extraction"""
    q0 = text or ""
    q1 = clean_text_min(text or "")
    brands = extract_brands(text or "")
    q2 = " ; ".join(sorted(set(brands))) if brands else ""
    qs = [q for q in [q0, q1, q2] if q]
    return qs

def match_post_smart(text: str, index: dict, threshold=0.74, topk=5, return_df=False):
    """
    - text: New Post Content
    - index: The theme_index constructed above
    - threshold: Hit rate threshold (MPNet: 0.74–0.78; MiniLM: 0.70–0.75 recommended)
    - topk: Return the top K candidates
    """
    if not text or not text.strip():
        print("Empty text.");
        return None

    themes = index["themes"]; topics = index["topic_entries"]
    queries = _make_queries(text)
    q_embs  = embed_texts(queries, show_progress=False)

    rows = []
    for t in themes:
        # Center of Mass of the Subject
        c = t["centroid"]
        # Maximum similarity to this centroid during the query
        best_q = float(np.max(np.dot(q_embs, c)))
        # Additionally: Recalculate the center of mass for some “sample monthly topics” related to this theme to prevent excessive averaging of the center of mass.
        extra = []
        for ei in t["examples"][:3]:  # Take up to 3 examples
            ed = topics[ei].get("emb_docs")
            if ed is not None:
                extra.append(float(np.max(np.dot(q_embs, ed))))
        if extra:
            best_q = max(best_q, max(extra))
        # Collect and display information
        ex_idx = t["examples"][0]
        label  = topics[ex_idx]["label"]
        rows.append({
            "theme_id": t["id"],
            "score": best_q,
            "first_month": t["first_month"],
            "recur_months": ", ".join([m for m in t["months"][1:]]),
            "months_count": len(t["months"]),
            "example_label": label,
            "brands": ", ".join(sorted(t["brands"])) if t["brands"] else ""
        })
    df = pd.DataFrame(rows).sort_values("score", ascending=False)

    hits = df[df["score"] >= threshold].head(topk)
    if hits.empty:
        print(f"No (max={df['score'].max():.3f} < threshold {threshold}). "
              f"Recommendation: Lower the threshold to {threshold-0.02:.2f}~{threshold-0.04:.2f}，or verify whether the index contains the relevant month。")
        if return_df:
            return df.head(topk)
        return None

    for i, r in hits.iterrows():
        print(f"[MATCH] score={r['score']:.3f}  theme#{int(r['theme_id'])}")
        print("  First seen:", r["first_month"])
        print("  Reappears in:", r["recur_months"] if r["recur_months"] else "(none)")
        print("  Months covered:", int(r["months_count"]))
        print("  Example label:", r["example_label"])
        if r["brands"]:
            print("  Brand cues:", r["brands"])
        print("-")
    if return_df:
        return hits

# ==== Cell 4: example ====

post = """
Scam Number:8557440611
Scammer’s Website or Email: Only Display Number
Additional information about this scam: Screenshot attached below. They trying to sell prime Tv subscriptions
"""
match_post_smart(post, theme_index, threshold=0.75, topk=3)  #  Adjustable Threshold and Return Count

# ==== Cell 5: timeline utilities & plotter ====
import os, re
import numpy as np
import pandas as pd
import plotly.graph_objects as go


OUT_TRACK_DIR = os.path.join(OUT_BASE, "_THEME_TRACKING_")
os.makedirs(OUT_TRACK_DIR, exist_ok=True)

def _to_month_dt(s):
    return pd.to_datetime(s, format="%Y-%m", errors="coerce")

def _timeline_for_theme(post_text: str, theme: dict, topics: list, prefer_docs=True):
    """Given a post and a topic, return a DataFrame containing the maximum similarity scores for each month."""
    # Generate three-view queries: Original / Streamlined / Branded
    queries = _make_queries(post_text)
    q_embs  = embed_texts(queries, show_progress=False)  # (Q, D)

    # Multiple “Month-Topic” instances may exist for the same topic within a single month → Aggregate monthly and take the highest score
    by_month = {}
    for ei in theme["examples"]:
        e = topics[ei]   #  month / emb_label / emb_docs / label ...
        m = e["month"]
        by_month.setdefault(m, []).append(e)

    pts = []
    for m, items in by_month.items():
        sims = []
        for e in items:

            if prefer_docs and e.get("emb_docs") is not None:
                sims.append(float(np.max(np.dot(q_embs, e["emb_docs"]))))
            sims.append(float(np.max(np.dot(q_embs, e["emb_label"]))))
        pts.append({"month": m, "score": max(sims)})

    df = pd.DataFrame(pts)
    if df.empty:
        return df
    df["month_dt"] = _to_month_dt(df["month"])
    df = df.sort_values("month_dt")
    return df

def plot_post_timeline(post_text: str,
                       index: dict,
                       topk: int = 1,
                       threshold: float = 0.75,
                       save_html: str | None = None):
    """Plot a monthly similarity curve for the top-k candidate topics of the post and output the matching summaries."""
    themes = index["themes"]; topics = index["topic_entries"]

    # 1) First, rank candidate topics using the center of mass + a small sample of documents (consistent with your matching logic).
    queries = _make_queries(post_text)
    q_embs  = embed_texts(queries, show_progress=False)
    rows = []
    for t in themes:
        c = t["centroid"]
        best = float(np.max(np.dot(q_embs, c)))
        for ei in t["examples"][:3]:
            ed = topics[ei].get("emb_docs")
            if ed is not None:
                best = max(best, float(np.max(np.dot(q_embs, ed))))
        ex_idx = t["examples"][0]
        rows.append({"theme_id": t["id"], "score": best, "example_label": topics[ex_idx]["label"]})
    cand = pd.DataFrame(rows).sort_values("score", ascending=False).head(max(1, topk))

    # 2) Calculate the “monthly similarity” for each candidate topic individually.
    fig = go.Figure()
    summaries = []
    for _, r in cand.iterrows():
        t = next(tt for tt in themes if tt["id"] == int(r["theme_id"]))
        df_line = _timeline_for_theme(post_text, t, topics)
        if df_line.empty:
            continue
        fig.add_trace(go.Scatter(
            x=df_line["month_dt"], y=df_line["score"],
            mode="lines+markers",
            name=f"theme#{int(r['theme_id'])}",
            hovertext=df_line["month"]
        ))
        # Statistical Hit Month
        hits = df_line[df_line["score"] >= threshold]["month"].tolist()
        summaries.append({
            "theme_id": int(r["theme_id"]),
            "max_score": float(df_line["score"].max()),
            "first_seen": df_line["month"].min(),
            "hit_months": hits
        })

    # 3) Threshold Line + Axis Settings
    fig.add_hline(y=threshold, line_dash="dash", line_color="gray", annotation_text="threshold")
    fig.update_layout(
        title=f"Post → Theme timeline similarity (top{len(cand)}); threshold={threshold}",
        xaxis_title="Month", yaxis_title="Cosine similarity",
        yaxis=dict(range=[0,1.0])
    )

    # 4) Save HTML
    if save_html is None:
        slug = re.sub(r"[^a-z0-9]+", "-", clean_text_min(post_text))[:60].strip("-") or "post"
        save_html = os.path.join(OUT_TRACK_DIR, f"timeline_{slug}.html")
    fig.write_html(save_html)
    print("Saved HTML:", save_html)
    fig.show()

    # 5) Text Summary
    for s in summaries:
        print(f"\nTheme #{s['theme_id']}: max={s['max_score']:.3f}")
        print("  First seen by time:", s["first_seen"])
        print("  ≥ threshold in:", s["hit_months"] if s["hit_months"] else "(none)")

    return cand

import os, pandas as pd

OUT_TRACK_DIR = os.path.join(OUT_BASE, "_THEME_TRACKING_")
os.makedirs(OUT_TRACK_DIR, exist_ok=True)

theme_table = pd.read_csv(os.path.join(OUT_TRACK_DIR, "month_topic_to_theme.csv"))

theme_table.to_csv(os.path.join(OUT_TRACK_DIR, "theme_map.csv"),
                   index=False, encoding="utf-8-sig")
print("Saved:", os.path.join(OUT_TRACK_DIR, "theme_map.csv"))

import os
OUT_TRACK_DIR = os.path.join(OUT_BASE, "_THEME_TRACKING_")
os.makedirs(OUT_TRACK_DIR, exist_ok=True)
theme_table.to_csv(os.path.join(OUT_TRACK_DIR, "theme_map.csv"),
                   index=False, encoding="utf-8-sig")
print("Saved:", os.path.join(OUT_TRACK_DIR, "theme_map.csv"))

# ==== Cell 6: examples ====
post_demo_1 = """
Invoice No: 50330-6369
We've noticed an unusual charge on your PayPal LLC account for 453.30 USD.
If it’s unauthorized, call our support immediately to cancel. Tracking No: 6078352-6868.
"""
plot_post_timeline(post_demo_1, theme_index, topk=1, threshold=0.75)

post_demo_2 = """
Scam Number: 8557440611
Additional info: They try to sell Prime TV subscriptions / PrimeVideo activation.
"""
plot_post_timeline(post_demo_2, theme_index, topk=3, threshold=0.75)

#If no
# ==== Config ====
OUT_BASE       = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
STORE_DIR      = f"{OUT_BASE}/_THEME_MATCHER_"
THEME_SIM_THR  = 0.78   # Threshold for assigning new posts to existing topics (similarity ≥ this value indicates belonging to that topic)
OUTLIER_SIM_THR= 0.80   # Threshold for merging “outliers of the same type” within outlier clusters (higher values indicate stricter criteria)
PROMOTE_K      = 10     # When K outliers of the same type are detected, they are promoted to a new topic.
ENCODER_NAME   = "all-mpnet-base-v2"  # Suitable for theme-level semantic matching

import os, re, json, pickle, pathlib, math
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer

pathlib.Path(STORE_DIR).mkdir(parents=True, exist_ok=True)

# ==== Encoder ====
if "sbert_matcher" not in globals():
    sbert_matcher = SentenceTransformer(ENCODER_NAME)

def embed_texts(texts, batch_size=128):
    return sbert_matcher.encode(texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True)

def cos_sim_one_to_many(vec, mat):
    """vec: (d,), mat: (n,d) -> (n,) cosine, since it is already normalized, the dot product suffices."""
    return mat @ vec

def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

def label_from_top_words(s: str, k=6):
    if not isinstance(s, str): return ""
    toks = [w.strip() for w in s.split(",") if w.strip()]
    DROP = {"hellip","amp","nbsp","quot"}
    out, seen = [], set()
    for w in toks:
        wl = w.lower()
        if wl in DROP or wl in seen:
            continue
        out.append(w); seen.add(wl)
        if len(out) >= k: break
    return " ; ".join(out) if out else ""

def build_or_load_theme_db(sim_thr=0.78):
    db_pkl = f"{STORE_DIR}/THEME_DB.pkl"
    if os.path.exists(db_pkl):
        with open(db_pkl, "rb") as f:
            return pickle.load(f)

    # 1) Collect topic labels and embeddings for all months
    month_dirs = sorted([d for d in os.listdir(OUT_BASE) if re.fullmatch(r"\d{4}-\d{2}", d)], key=month_key)
    entries = []  # Each line {month, label, size, emb}
    for m in month_dirs:
        rep_csv = os.path.join(OUT_BASE, m, "topic_representations.csv")
        if not os.path.exists(rep_csv):
            continue
        df = pd.read_csv(rep_csv)
        if "top_words" not in df.columns:
            for col in ["Words", "Representation"]:
                if col in df.columns:
                    df = df.rename(columns={col: "top_words"})
                    break
        if "top_words" not in df.columns:
            continue
        if "size" not in df.columns and "Count" in df.columns:
            df = df.rename(columns={"Count": "size"})
        if "size" not in df.columns:
            df["size"] = 0

        df["label"] = df["top_words"].apply(label_from_top_words)
        df = df[df["label"].str.len() > 0]
        if df.empty:
            continue

        embs = embed_texts(df["label"].tolist())
        for (_, row), emb in zip(df.iterrows(), embs):
            entries.append({
                "month": m,
                "label": row["label"],
                "size": int(row["size"]),
                "emb": emb.astype(np.float32)
            })

    if not entries:
        raise RuntimeError("No monthly topic_representations found under OUT_BASE.")

    # 2) Streaming Merge into “Global Topic”: Leader Clustering on Embeddings
    themes = []  # list of dict: {id, centroid, months, labels, total_size}
    next_id = 0

    def add_theme(e):
        nonlocal next_id
        t = {
            "id": next_id,
            "centroid": e["emb"].copy(),
            "months": [e["month"]],
            "labels": [e["label"]],
            "total_size": e["size"]
        }
        themes.append(t)
        next_id += 1
        return t

    # Sort by month and time to ensure consistency in timing.
    entries_sorted = sorted(entries, key=lambda x: month_key(x["month"]))
    for e in entries_sorted:
        if not themes:
            add_theme(e); continue
        cents = np.vstack([t["centroid"] for t in themes])  # (T, d)
        sims  = cos_sim_one_to_many(e["emb"], cents)        # (T,)
        j = int(np.argmax(sims)); best = float(sims[j])
        if best >= sim_thr:
            t = themes[j]
            n = len(t["labels"])
            t["centroid"] = (t["centroid"]*n + e["emb"]) / (n+1)
            if e["month"] != t["months"][-1]:
                t["months"].append(e["month"])
            t["labels"].append(e["label"])
            t["total_size"] += e["size"]
        else:
            add_theme(e)

    # 3) Aggregate DataFrame + Centroid Matrix
    df = []
    for t in themes:
        months = sorted(set(t["months"]), key=month_key)
        df.append({
            "theme_id": t["id"],
            "first_month": months[0],
            "recur_months": ", ".join(months[1:]),
            "months_count": len(months),
            "example_label": t["labels"][0] if t["labels"] else "",
            "sum_docs": t["total_size"]
        })
    theme_df = pd.DataFrame(df).sort_values(["first_month","months_count","sum_docs"], ascending=[True, False, False])
    centroids = np.vstack([t["centroid"] for t in themes]).astype(np.float32)

    theme_db = {
        "encoder": ENCODER_NAME,
        "sim_thr": sim_thr,
        "theme_df": theme_df,
        "centroids": centroids
    }
    with open(db_pkl, "wb") as f:
        pickle.dump(theme_db, f)
    theme_df.to_csv(f"{STORE_DIR}/THEME_DB.csv", index=False, encoding="utf-8-sig")
    print(f"Built themes: {len(theme_df)}. Saved THEME_DB to {STORE_DIR}")
    return theme_db

theme_db = build_or_load_theme_db(sim_thr=THEME_SIM_THR)
print(f"Loaded theme_db: {len(theme_db['theme_df'])} themes; encoder={theme_db['encoder']}")

class OutlierManager:
    def __init__(self, store_dir, outlier_sim_thr=0.80, promote_k=10):
        self.store_dir = store_dir
        self.outlier_sim_thr = outlier_sim_thr
        self.promote_k = promote_k
        self._load_state()

    def _paths(self):
        return (f"{self.store_dir}/OUTLIERS.pkl",
                f"{self.store_dir}/OUTLIER_CLUSTERS.pkl",
                f"{self.store_dir}/THEME_DB.pkl")

    def _load_state(self):
        out_pkl, clu_pkl, theme_pkl = self._paths()
        self.outliers = []  # list of dict: {id, month, text, emb, cluster_id}
        self.clusters = []  # list of dict: {id, centroid, size, months, texts_idx}
        if os.path.exists(out_pkl):
            with open(out_pkl, "rb") as f:
                self.outliers = pickle.load(f)
        if os.path.exists(clu_pkl):
            with open(clu_pkl, "rb") as f:
                self.clusters = pickle.load(f)

    def _save_state(self):
        out_pkl, clu_pkl, _ = self._paths()
        with open(out_pkl, "wb") as f:
            pickle.dump(self.outliers, f)
        with open(clu_pkl, "wb") as f:
            pickle.dump(self.clusters, f)

    def _add_cluster(self, emb, month, outlier_idx):
        cid = 0 if not self.clusters else (max(c["id"] for c in self.clusters) + 1)
        self.clusters.append({
            "id": cid,
            "centroid": emb.copy(),
            "size": 1,
            "months": [month],
            "texts_idx": [outlier_idx],
        })
        return cid

    def add_outlier(self, text, month):
        emb = embed_texts([text])[0].astype(np.float32)
        oid = 0 if not self.outliers else (max(o["id"] for o in self.outliers) + 1)
        rec = {"id": oid, "month": month, "text": text, "emb": emb, "cluster_id": None}
        # Merge into outlier clusters
        if self.clusters:
            cents = np.vstack([c["centroid"] for c in self.clusters])
            sims  = cos_sim_one_to_many(emb, cents)
            j = int(np.argmax(sims)); best = float(sims[j])
            if best >= self.outlier_sim_thr:
                c = self.clusters[j]
                n = c["size"]
                c["centroid"] = (c["centroid"]*n + emb) / (n+1)
                c["size"] += 1
                if month != c["months"][-1]:
                    c["months"].append(month)
                c["texts_idx"].append(oid)
                rec["cluster_id"] = c["id"]
                self.outliers.append(rec); self._save_state()
                return rec, c, False  # added, cluster, promoted=False
        # Otherwise, create a new cluster.
        cid = self._add_cluster(emb, month, oid)
        rec["cluster_id"] = cid
        self.outliers.append(rec); self._save_state()
        return rec, [c for c in self.clusters if c["id"]==cid][0], False

    def maybe_promote(self, cluster_id, theme_db):
        """Otherwise, when the new cluster size is ≥ K, promote it to a new topic and return the new topic information or None."""
        cands = [c for c in self.clusters if c["id"] == cluster_id]
        if not cands: return None
        c = cands[0]
        if c["size"] < self.promote_k:
            return None

        # Generate topic tags (extract 5 keywords/phrases from clustered text using simple TF-IDF)
        texts = [o["text"] for o in self.outliers if o["id"] in c["texts_idx"]]
        vec = CountVectorizer(stop_words="english", ngram_range=(1,2), min_df=1, max_df=0.9)
        X = vec.fit_transform(texts)  # (n, V)
        # Take the mean TF (simplified version of “core word”）
        tf = np.asarray(X.mean(axis=0)).ravel()
        vocab = np.array(sorted(vec.vocabulary_.items(), key=lambda kv: kv[1]))[:,0]
        topk = min(5, len(tf))
        kw = vocab[np.argsort(-tf)[:topk]].tolist()
        label = " ; ".join(kw)

        # Enter a new theme and write it back to THEME_DB
        new_centroid = c["centroid"].astype(np.float32)
        theme_df = theme_db["theme_df"].copy()
        centroids = theme_db["centroids"]
        new_id = int(theme_df["theme_id"].max()) + 1 if not theme_df.empty else 0
        months = sorted(c["months"], key=month_key)

        new_row = pd.DataFrame([{
            "theme_id": new_id,
            "first_month": months[0],
            "recur_months": ", ".join(months[1:]),
            "months_count": len(months),
            "example_label": label,
            "sum_docs": c["size"]
        }])
        theme_df = pd.concat([theme_df, new_row], ignore_index=True)
        centroids = np.vstack([centroids, new_centroid]) if centroids.size else new_centroid[None,:]

        # Save
        theme_db["theme_df"] = theme_df
        theme_db["centroids"] = centroids
        with open(f"{STORE_DIR}/THEME_DB.pkl", "wb") as f:
            pickle.dump(theme_db, f)
        theme_df.to_csv(f"{STORE_DIR}/THEME_DB.csv", index=False, encoding="utf-8-sig")

        return {
            "theme_id": new_id,
            "label": label,
            "first_month": months[0],
            "recur_months": ", ".join(months[1:]),
            "months_count": len(months),
            "size": c["size"]
        }

outlier_mgr = OutlierManager(STORE_DIR, outlier_sim_thr=OUTLIER_SIM_THR, promote_k=PROMOTE_K)
print("Outlier manager ready.",
      f"clusters={len(outlier_mgr.clusters)}; outliers={len(outlier_mgr.outliers)}")

def assign_post_or_outlier(post_text: str, month_str: str,
                           theme_db: dict,
                           theme_thr: float = THEME_SIM_THR,
                           verbose: bool = True):
    """Returns:
- If matched: dict(type="match", theme_id, score, first_month, recur_months, months_count, example_label)
- If outlier: dict(type="outlier", cluster_id, cluster_size, promoted=?new_theme_info or None)
"""

    # 1) First, try to categorize it under an existing theme.
    vec = embed_texts([post_text])[0].astype(np.float32)
    cents = theme_db["centroids"]
    if cents.size:
        sims = cos_sim_one_to_many(vec, cents)
        j = int(np.argmax(sims)); best = float(sims[j])
        if best >= theme_thr:
            row = theme_db["theme_df"].iloc[j]
            res = {
                "type": "match",
                "theme_id": int(row["theme_id"]),
                "score": best,
                "first_month": row["first_month"],
                "recur_months": row["recur_months"],
                "months_count": int(row["months_count"]),
                "example_label": row["example_label"],
            }
            if verbose:
                print(f"[MATCH] score={best:.3f}  theme#{res['theme_id']}")
                print("  First seen:", res["first_month"])
                print("  Reappears in:", res["recur_months"] if res["recur_months"] else "(none)")
                print("  Months covered:", res["months_count"])
                print("  Example label:", res["example_label"])
            return res

    # 2) Enter the outlier pool Check if it merges + Check if it promotes to a new topic
    rec, clu, _ = outlier_mgr.add_outlier(post_text, month_str)
    promoted = outlier_mgr.maybe_promote(rec["cluster_id"], theme_db)
    res = {
        "type": "outlier",
        "cluster_id": rec["cluster_id"],
        "cluster_size": clu["size"],
        "promoted": promoted
    }
    if verbose:
        print(f"[OUTLIER] cluster#{rec['cluster_id']}  size={clu['size']}")
        if promoted:
            print("[PROMOTED] New theme created:",
                  f"theme#{promoted['theme_id']}  size={promoted['size']}")
            print("  First seen:", promoted["first_month"])
            print("  Reappears in:", promoted["recur_months"] if promoted["recur_months"] else "(none)")
            print("  Months covered:", promoted["months_count"])
            print("  Example label:", promoted["label"])
        else:
            need = max(0, PROMOTE_K - clu["size"])
            print(f"  Not enough to promote. Need +{need} similar outliers to reach K={PROMOTE_K}.")
    return res

posts = [
    ("2024-11", "Your Norton subscription was renewed for $349. Call to cancel."),
    ("2024-11", "Microsoft Defender alert detected a virus. Contact support."),
    ("2024-12", "Geek Squad invoice posted. To get a refund, call the number."),
]

results = []
for m, text in posts:
    results.append(assign_post_or_outlier(text, m, theme_db, theme_thr=THEME_SIM_THR, verbose=False))

for r in results:
    if r["type"] == "match":
        print(f"[MATCH] theme#{r['theme_id']}  score={r['score']:.3f}  first={r['first_month']}  recur={r['recur_months'] or '(none)'}  label={r['example_label']}")
    else:
        print(f"[OUTLIER] cluster#{r['cluster_id']}  size={r['cluster_size']}  promoted={bool(r['promoted'])}")

# ==== FAST MODE: config & helpers (CPU friendly) ====
import os, re, json, pathlib
import numpy as np
import pandas as pd

OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"


MAKE_HTML  = True
MAKE_PNG   = False
IMG_WIDTH  = 1400
IMG_HEIGHT = 900
IMG_SCALE  = 2
MAX_DOCS_SCATTER = 1200

# ——Encoder: MiniLM (3–5× faster than mpnet on CPU)——
from sentence_transformers import SentenceTransformer
try:
    sbert_fast = SentenceTransformer("all-MiniLM-L6-v2")
except Exception:

    sbert_fast = SentenceTransformer("all-MiniLM-L12-v2")

def embed_texts(texts, batch_size=256, show_progress=True):
    return sbert_fast.encode(
        texts, batch_size=batch_size, show_progress_bar=show_progress, normalize_embeddings=True
    )

# ——Lightweight BERTopic Builder (Faster UMAP/HDBSCAN/Vectorizer)——
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as ENG_SW
from bertopic import BERTopic
import umap, hdbscan

CUSTOM_STOP = {
    "s","t","amp","utm","nbsp","indexhtml","presentphp","primepreloaderhtml","php","html","preloader","loader",
    "used","domain","domains","registrant","pm","est","uid","hxxp",
    "http","https","www","number","numbers",
    "extra","info","information","additional",
    "email","emails","telephone",
    "scam","scams","scammer","scammers","website","websites"
}

def build_topic_model_fast(n_docs: int, target_topics=None):
    # Smaller, Faster Word Bags: Limit max_features to just binary phrases
    max_feats = 30000 if n_docs > 2000 else 20000
    min_ratio = max(2 / max(n_docs, 1), 0.001)
    vectorizer = CountVectorizer(
        stop_words=list(set(ENG_SW) | CUSTOM_STOP),
        ngram_range=(1, 2),
        max_features=max_feats,
        min_df=min_ratio, max_df=0.9,
        token_pattern=r"(?u)\b[a-z0-9_]+\b"
    )

    # UMAP Minimal: Low-dimensional, Low-neighbor, low_memory
    umap_model = umap.UMAP(
        n_neighbors=10, n_components=5, min_dist=0.0,
        metric="cosine", random_state=42, low_memory=True
    )

    # HDBSCAN Faster: Larger min_cluster_size, prediction_data disabled
    mcs = max(40, int(0.03 * n_docs))
    ms  = max(5,  int(0.01 * n_docs))
    hdbscan_model = hdbscan.HDBSCAN(
        min_cluster_size=mcs, min_samples=ms,
        cluster_selection_method="eom",
        prediction_data=False, core_dist_n_jobs=1
    )

    tm = BERTopic(
        embedding_model=None,
        vectorizer_model=vectorizer,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        language="english",
        top_n_words=8,
        calculate_probabilities=False,
        nr_topics=target_topics,
        verbose=False
    )
    return tm

def _save_fig(fig, out_dir, name):
    import os
    path_html = os.path.join(out_dir, f"{name}.html")
    path_png  = os.path.join(out_dir, f"{name}.png")
    if MAKE_HTML:
        fig.write_html(path_html)
    if MAKE_PNG:

        fig.write_image(path_png, width=IMG_WIDTH, height=IMG_HEIGHT, scale=IMG_SCALE)

def _maybe(fig_func, *args, out_dir=None, name=None, **kwargs):
    try:
        fig = fig_func(*args, **kwargs)
        _save_fig(fig, out_dir, name)
        return True
    except Exception as e:
        print(f"[skip {name}] {e}")
        return False

# ==== FAST monthly modeling & export (HTML only, with embedding cache) ====
import os, pathlib, numpy as np

def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

assert 'df_bt' in globals() and 'docs_bt' in globals(), "df_bt / docs_bt is already in memory"

for month_str, g in df_bt.groupby("month_str"):
    if pd.isna(month_str):
        print("[skip NaT month]");
        continue

    idx = g.index.to_numpy()
    sub_docs = [docs_bt[i] for i in idx]
    n_docs   = len(sub_docs)
    if n_docs < 3:
        print(f"[{month_str}] docs={n_docs} < 3 → skip");
        continue

    out_dir = os.path.join(OUT_BASE, month_str)
    pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)

    # /month/embeddings.npy
    cache_emb = os.path.join(out_dir, "embeddings.npy")
    if os.path.exists(cache_emb):
        sub_emb = np.load(cache_emb)
        if sub_emb.shape[0] != n_docs:
            sub_emb = embed_texts(sub_docs, show_progress=True)
            np.save(cache_emb, sub_emb)
    else:
        sub_emb = embed_texts(sub_docs, show_progress=True)
        np.save(cache_emb, sub_emb)

    tm = build_topic_model_fast(n_docs, target_topics=None)
    topics, probs = tm.fit_transform(sub_docs, embeddings=sub_emb)

  # Export: info / doc saved separately (for subsequent analysis and tracking)
    info_df = tm.get_topic_info()
    info_df.to_csv(os.path.join(out_dir, "topic_info.csv"), index=False, encoding="utf-8-sig")

    doc_df = tm.get_document_info(sub_docs)
    doc_df["orig_index"] = idx
    if "id" in g.columns: doc_df["id"] = g.loc[idx, "id"].values
    doc_df["created_at"] = g.loc[idx, "created_at"].values
    doc_df.to_csv(os.path.join(out_dir, "document_topics.csv"), index=False, encoding="utf-8-sig")

    # Visualization (HTML export only, with controlled scale)
    n_topics = int((info_df["Topic"] != -1).sum())
    print(f"[{month_str}] topics(≠-1)={n_topics}, docs={n_docs}")

    _maybe(tm.visualize_hierarchy, out_dir=out_dir, name="hierarchy")
    if n_topics >= 2:
        _maybe(tm.visualize_heatmap, top_n_topics=min(15, n_topics), out_dir=out_dir, name="heatmap")

    # Document Scatter (Smaller Sampling, Faster Processing)
    if n_docs > MAX_DOCS_SCATTER:
        sel = np.random.choice(n_docs, MAX_DOCS_SCATTER, replace=False)
        docs_vis = [sub_docs[i] for i in sel]
        emb_vis  = sub_emb[sel]
    else:
        docs_vis = sub_docs
        emb_vis  = sub_emb
    _maybe(tm.visualize_documents, docs_vis, embeddings=emb_vis, out_dir=out_dir, name="documents")

    _maybe(tm.visualize_barchart, top_n_topics=min(15, max(1, n_topics)), out_dir=out_dir, name="barchart")
    _maybe(tm.visualize_topics, out_dir=out_dir, name="topics")

print("Monthly HTML images have been generated (including cached embeddings).")

# ==== Summary plot from monthly outputs (no global model needed) ====
import os, re, pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

# Summarize the number of non-outlier themes each month
rows = []
months = [d for d in os.listdir(OUT_BASE) if re.fullmatch(r"\d{4}-\d{2}", d)]
for m in sorted(months, key=month_key):
    p = os.path.join(OUT_BASE, m, "topic_info.csv")
    if not os.path.exists(p):
        continue
    df = pd.read_csv(p)
    if "Topic" not in df.columns:
        continue
    n_topics = int((df["Topic"] != -1).sum())
    rows.append({"month": m, "n_topics": n_topics})

if not rows:
    raise RuntimeError("error")

summary = pd.DataFrame(rows).sort_values("month")
summary["month_dt"] = pd.to_datetime(summary["month"] + "-01")
summary["n_topics_ma3"] = summary["n_topics"].rolling(3, min_periods=1).mean()

fig = make_subplots(specs=[[{"secondary_y": True}]])
fig.add_trace(go.Scatter(x=summary["month_dt"], y=summary["n_topics"],
                         mode="lines+markers", name="#Topics / month"))
fig.add_trace(go.Scatter(x=summary["month_dt"], y=summary["n_topics_ma3"],
                         mode="lines", name="3-mo moving avg"), secondary_y=True)
fig.update_layout(title="Monthly Topic Counts (with 3-month MA)", xaxis_title="Month")
fig.show()


out_html = os.path.join(OUT_BASE, "_ALL_", "summary_topics_over_time.html")
pathlib.Path(os.path.dirname(out_html)).mkdir(parents=True, exist_ok=True)
fig.write_html(out_html)
print("out", out_html)

# === Cell A: Topics over Month (from monthly_summary.csv) ===
import os, re
import pandas as pd
import plotly.express as px


OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
OUT_ALL  = os.path.join(OUT_BASE, "_ALL_")
os.makedirs(OUT_ALL, exist_ok=True)

summary_csv = os.path.join(OUT_BASE, "monthly_summary.csv")
assert os.path.exists(summary_csv), f"cannot find {summary_csv}"

# Read and perform column name compatibility
df = pd.read_csv(summary_csv)

if "month" not in df.columns and "month_str" in df.columns:
    df = df.rename(columns={"month_str": "month"})
if "n_topics" not in df.columns and "topics" in df.columns:
    df = df.rename(columns={"topics": "n_topics"})
if "n_docs" not in df.columns and "docs" in df.columns:
    df = df.rename(columns={"docs": "n_docs"})

def _ok(m): return isinstance(m, str) and re.fullmatch(r"\d{4}-\d{2}", m) is not None
df = df[df["month"].apply(_ok)].copy()
df["month_dt"] = pd.to_datetime(df["month"], format="%Y-%m")
df = df.sort_values("month_dt").reset_index(drop=True)

# 3-Month Moving Average
df["topics_ma3"] = df["n_topics"].rolling(3, min_periods=1).mean()

# Draw a line graph
fig = px.line(df, x="month_dt", y="n_topics", markers=True,
              title="Topics over Month (from monthly_summary.csv)",
              labels={"month_dt":"Month", "n_topics":"#Topics"})
fig.add_scatter(x=df["month_dt"], y=df["topics_ma3"], mode="lines", name="3-mo MA",
                hovertemplate="%{y:.2f}")


out_html = os.path.join(OUT_ALL, "topics_over_month.html")
fig.write_html(out_html)
print("out：", out_html)

# === Cell B: Themes' presence over months (from _THEME_INDEX_/theme_timeline.csv) ===
import os, re
import pandas as pd
import plotly.graph_objects as go

OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
INDEX_DIR = os.path.join(OUT_BASE, "_THEME_INDEX_")
timeline_csv = os.path.join(INDEX_DIR, "theme_timeline.csv")
assert os.path.exists(timeline_csv), f"Not found {timeline_csv}"

tt = pd.read_csv(timeline_csv)

# Compatible Column Names
need_cols = {"theme_id","first_month","recur_months"}
missing = need_cols - set(tt.columns)
assert not missing, f"Missing column: {missing}"

# Expand the month of appearance for each theme (first + recur)
rows = []
for _, r in tt.iterrows():
    tid = int(r["theme_id"])
    first = str(r["first_month"])
    months = [first]
    rec = str(r.get("recur_months", "") or "")
    if rec.strip():
        months += [m.strip() for m in rec.split(",") if m.strip()]
    for m in months:
        if re.fullmatch(r"\d{4}-\d{2}", m):
            rows.append({"theme_id": tid, "month": m, "present": 1})

dfm = pd.DataFrame(rows)
assert not dfm.empty, "No available theme-month data."

# Rotate time and sort
dfm["month_dt"] = pd.to_datetime(dfm["month"], format="%Y-%m")
dfm = dfm.sort_values(["theme_id", "month_dt"])

# Select the top K topics (adjustable) covering the most months.
TOP_THEMES = 8
top_ids = (dfm.groupby("theme_id")["present"].sum()
           .sort_values(ascending=False).head(TOP_THEMES).index.tolist())

dfp = dfm[dfm["theme_id"].isin(top_ids)].copy()

# Fill in missing months as 0 (for easy connection)
all_months = pd.date_range(dfp["month_dt"].min(), dfp["month_dt"].max(), freq="MS")
plot_rows = []
for tid, g in dfp.groupby("theme_id"):
    mark = set(g["month_dt"].dt.to_period("M").astype(str))
    for m in all_months:
        plot_rows.append({"theme_id": tid, "month_dt": m,
                          "present": 1 if m.to_period("M").strftime("%Y-%m") in mark else 0})
wide = pd.DataFrame(plot_rows)

# Plot: Each line 0/1 indicates whether the event occurred that month.
fig = go.Figure()
for tid, g in wide.groupby("theme_id"):
    g = g.sort_values("month_dt")
    fig.add_trace(go.Scatter(
        x=g["month_dt"], y=g["present"],
        mode="lines+markers", name=f"theme#{tid}",
        hovertemplate="Month=%{x|%Y-%m}<br>Present=%{y}<extra></extra>"
    ))

fig.update_layout(
    title=f"Themes presence over months (top {TOP_THEMES})",
    xaxis_title="Month", yaxis_title="Present (0/1)",
    yaxis=dict(range=[-0.05, 1.15])
)

out_html2 = os.path.join(OUT_BASE, "_ALL_", "themes_presence_over_months.html")
fig.write_html(out_html2)
print("out", out_html2)

# ==== RQ : load theme index ====
import os, re, pandas as pd
import plotly.express as px
import plotly.graph_objects as go

OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
IDX_DIR  = os.path.join(OUT_BASE, "_THEME_INDEX_")
OUT_DIR  = os.path.join(OUT_BASE, "_RQ_RESULTS_")
os.makedirs(OUT_DIR, exist_ok=True)

def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

# 1) Month-Theme → Global Theme Mapping Details (Including size/label/sim, etc.)
map_csv  = os.path.join(IDX_DIR, "month_topic_to_theme.csv")
# 2) Monthly timeline for each global theme (columns typically include theme_id, month, size)
time_csv = os.path.join(IDX_DIR, "theme_timeline.csv")

assert os.path.exists(map_csv) and os.path.exists(time_csv), "The CSV file for _THEME_INDEX_ is missing. Please run Theme Alignment (Add-on H/H+) first."

df_map  = pd.read_csv(map_csv)
df_time = pd.read_csv(time_csv)

# Compatible Column Names
if "label" not in df_map.columns and "example_label" in df_map.columns:
    df_map = df_map.rename(columns={"example_label":"label"})
if "month" in df_time.columns:
    df_time["month"] = df_time["month"].astype(str)
    df_time = df_time.sort_values("month", key=lambda s: s.map(month_key))
df_map["month"] = df_map["month"].astype(str)

print(f"Loaded rows: map={len(df_map)}, timeline={len(df_time)}")

# === Prep cell: load df_map (mapping month-topic -> global theme) ===
import os, re, pandas as pd

OUT_BASE = "/content/drive/MyDrive/UCLFinal/bertopic_monthly"
OUT_TRACK_DIR = os.path.join(OUT_BASE, "_THEME_TRACKING_")
MAP_CSV = os.path.join(OUT_TRACK_DIR, "month_topic_to_theme.csv")

assert os.path.exists(MAP_CSV), f"Missing mapping file: {MAP_CSV}\nPlease run Topic Tracking (Add-on H/H+) first to generate it."

df_map = pd.read_csv(MAP_CSV)

# Column Type Standardization
df_map["month"] = df_map["month"].astype(str)
for c in ("theme_id","topic_id","size"):
    if c in df_map.columns:
        df_map[c] = pd.to_numeric(df_map[c], errors="coerce").fillna(0).astype(int)
if "label" in df_map.columns:
    df_map["label"] = df_map["label"].astype(str)

# Month sort key for RQ1 aggregation
def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))
import plotly.express as px
OUT_DIR = os.path.join(OUT_BASE, "_RQ")
os.makedirs(OUT_DIR, exist_ok=True)

# ==== Rebuild theme_map.csv from ALL monthly folders (2016→present) ====

import os, re, numpy as np, pandas as pd
from pathlib import Path


OUT_TRACK_DIR     = os.path.join(OUT_BASE, "_THEME_TRACKING_")
MAX_DOCS_PER_TOPIC= 5        # For each “month-theme,” select several representative documents to determine the center of mass.
GROUP_THRESHOLD   = 0.78     # Cross-Month Merge Threshold (Fine-tunable between 0.76 and 0.82)
MIN_TOPIC_SIZE    = 0        # Filter out minor topics; use 0 to skip filtering.

Path(OUT_TRACK_DIR).mkdir(parents=True, exist_ok=True)

# ---------- helpers ----------
def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

def label_from_top_words(s: str, k=6):
    if not isinstance(s, str): return ""
    toks = [w.strip() for w in s.split(",") if w.strip()]
    DROP = {"hellip","amp","nbsp","quot"}
    out, seen = [], set()
    for w in toks:
        wl = w.lower()
        if wl in DROP or w in seen:
            continue
        out.append(w); seen.add(w)
        if len(out) >= k: break
    return " ; ".join(out)

# encoder: prioritize MPNet; fall back to MiniLM if it fails
try:
    from sentence_transformers import SentenceTransformer
    _MODEL_NAME = "all-mpnet-base-v2"
    sbert = SentenceTransformer(_MODEL_NAME)
except Exception:
    from sentence_transformers import SentenceTransformer
    _MODEL_NAME = "all-MiniLM-L6-v2"
    sbert = SentenceTransformer(_MODEL_NAME)

def embed_texts(texts, batch_size=128):
    return sbert.encode(texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True)

# ---------- 1) collect monthly topics ----------
topic_entries = []  # dict(month, topic_id, size, label, emb_label, emb_docs)

month_dirs = sorted(
    [d for d in os.listdir(OUT_BASE) if re.fullmatch(r"\d{4}-\d{2}", d)],
    key=month_key
)

if not month_dirs:
    raise RuntimeError(f"No YYYY-MM monthly folders were found under {OUT_BASE}.")

for m in month_dirs:
    rep_csv = os.path.join(OUT_BASE, m, "topic_representations.csv")
    if not os.path.exists(rep_csv):
        print(f"[skip {m}] no topic_representations.csv")
        continue
    rep = pd.read_csv(rep_csv)

    # Compatible Column Names
    if "top_words" not in rep.columns:
        for col in ["Words","Representation"]:
            if col in rep.columns:
                rep = rep.rename(columns={col: "top_words"})
                break
    if "topic_id" not in rep.columns and "Topic" in rep.columns:
        rep = rep.rename(columns={"Topic":"topic_id"})
    if "size" not in rep.columns and "Count" in rep.columns:
        rep = rep.rename(columns={"Count":"size"})
    if "size" not in rep.columns:
        rep["size"] = 0

    # Filter Outliers & Minor Themes
    rep = rep[rep["topic_id"] != -1].copy()
    if MIN_TOPIC_SIZE > 0:
        rep = rep[rep["size"] >= MIN_TOPIC_SIZE]
    if rep.empty:
        continue

    # Generate tags and encode
    rep["label"] = rep["top_words"].astype(str).apply(label_from_top_words)
    rep = rep[rep["label"].str.len() > 0]
    if rep.empty:
        continue

    emb_labels = embed_texts(rep["label"].tolist())

    # Optional: Represents the document's center of mass
    doc_csv = os.path.join(OUT_BASE, m, "document_topics.csv")
    docs_df = None
    if os.path.exists(doc_csv):
        dtmp = pd.read_csv(doc_csv)
        if "Topic" in dtmp.columns and "topic_id" not in dtmp.columns:
            dtmp = dtmp.rename(columns={"Topic":"topic_id"})
        if "Document" in dtmp.columns:
            docs_df = dtmp

    for i, r in rep.reset_index(drop=True).iterrows():
        tid   = int(r["topic_id"])
        size  = int(r["size"])
        label = r["label"]
        emb_l = emb_labels[i].astype(np.float32)

        emb_d = None
        if docs_df is not None:
            dsub = docs_df[docs_df["topic_id"] == tid]
            if "Probability" in dsub.columns:
                dsub = dsub.sort_values("Probability", ascending=False)
            texts = dsub["Document"].astype(str).head(MAX_DOCS_PER_TOPIC).tolist()
            if texts:
                ve = embed_texts(texts).mean(axis=0)
                emb_d = (ve / (np.linalg.norm(ve) + 1e-9)).astype(np.float32)

        topic_entries.append({
            "month": m, "topic_id": tid, "size": size, "label": label,
            "emb_label": emb_l, "emb_docs": emb_d
        })

print(f"Collected monthly topics: {len(topic_entries)} from {len(month_dirs)} months. Model = {_MODEL_NAME}")
if not topic_entries:
    raise RuntimeError("No monthly topic entries available (check if topic_representations.csv exists for each month and is not empty)")

# ---------- 2) merge across months into global themes ----------
themes = []  # list of dict(id, centroid, months(set), examples(list of idx), first_month)
assign_rows = []  # The ownership record for each “Month-Theme”
next_id = 0

def theme_sim(centroid, e):
    sims = [float(np.dot(centroid, e["emb_label"]))]
    if e["emb_docs"] is not None:
        sims.append(float(np.dot(centroid, e["emb_docs"])))
    return max(sims)

def update_centroid(cur, e):
    add = e["emb_label"]
    if e["emb_docs"] is not None:
        add = 0.5*e["emb_label"] + 0.5*e["emb_docs"]
        add = add / (np.linalg.norm(add) + 1e-9)
    new = cur + add
    return new / (np.linalg.norm(new) + 1e-9)

def add_theme(e):
    global next_id
    c = e["emb_label"].copy()
    themes.append({
        "id": next_id,
        "centroid": c,
        "months": {e["month"]},
        "examples": [e],
        "first_month": e["month"]
    })
    tid = next_id
    next_id += 1
    return tid, 1.0

# Chronological Order
order = sorted(range(len(topic_entries)), key=lambda i: month_key(topic_entries[i]["month"]))
for i in order:
    e = topic_entries[i]
    if not themes:
        tid, sim = add_theme(e)
        assign_rows.append({"month": e["month"], "topic_id": e["topic_id"], "size": e["size"],
                            "label": e["label"], "theme_id": tid, "sim": sim})
        continue

    cents = np.vstack([t["centroid"] for t in themes])
    sims  = np.dot(cents, e["emb_label"])
    if e["emb_docs"] is not None:
        sims = np.maximum(sims, np.dot(cents, e["emb_docs"]))
    j = int(np.argmax(sims)); best = float(sims[j])

    if best >= GROUP_THRESHOLD:
        t = themes[j]
        t["months"].add(e["month"])
        t["examples"].append(e)
        t["centroid"] = update_centroid(t["centroid"], e)
        assign_rows.append({"month": e["month"], "topic_id": e["topic_id"], "size": e["size"],
                            "label": e["label"], "theme_id": t["id"], "sim": best})
    else:
        tid, sim = add_theme(e)
        assign_rows.append({"month": e["month"], "topic_id": e["topic_id"], "size": e["size"],
                            "label": e["label"], "theme_id": tid, "sim": sim})

# Add the first month for each topic
for t in themes:
    t["months"] = sorted(list(t["months"]), key=month_key)
    t["first_month"] = t["months"][0]

assign_df = pd.DataFrame(assign_rows)
assign_df = assign_df.sort_values(["month","theme_id","topic_id"], key=lambda s: s.map(lambda x: month_key(x) if s.name=="month" else x))

start_month = assign_df["month"].min()
end_month   = assign_df["month"].max()
print(f"Built {len(themes)} global themes. Range: {start_month} → {end_month}")

# ---------- 3) save mapping (overwrite) ----------
map_path_a = os.path.join(OUT_TRACK_DIR, "theme_map.csv")
map_path_b = os.path.join(OUT_TRACK_DIR, "month_topic_to_theme.csv")

assign_df.to_csv(map_path_a, index=False, encoding="utf-8-sig")
assign_df.to_csv(map_path_b, index=False, encoding="utf-8-sig")

print("Saved mapping to:")
print("  ", map_path_a)
print("  ", map_path_b)

# ==== RQ1: Which topics are discussed most frequently? (2016→present) ====

import os, re, pandas as pd
import plotly.express as px

def month_key(s: str):
    m = re.fullmatch(r"(\d{4})-(\d{2})", str(s))
    return (9999, 99) if not m else (int(m.group(1)), int(m.group(2)))

OUT_TRACK_DIR = os.path.join(OUT_BASE, "_THEME_TRACKING_")
OUT_DIR = os.path.join(OUT_BASE, "_RQ_outputs")
os.makedirs(OUT_DIR, exist_ok=True)

path_a = os.path.join(OUT_TRACK_DIR, "theme_map.csv")
path_b = os.path.join(OUT_TRACK_DIR, "month_topic_to_theme.csv")
map_path = path_a if os.path.exists(path_a) else path_b

df_map = pd.read_csv(map_path)

# Clean up the month column to ensure it follows the YYYY-MM format
df_map["month"] = df_map["month"].astype(str)
df_map = df_map[df_map["month"].str.match(r"^\d{4}-\d{2}$", na=False)].copy()

# Automatically find the earliest month (for chart titles)
start_month = sorted(df_map["month"].unique(), key=month_key)[0]

# Aggregation: Total posts per global topic, months covered, first appearance month, sample tags
agg = (df_map
       .groupby("theme_id", as_index=False)
       .agg(total_docs=("size","sum"),
            months_covered=("month", lambda x: len(set(x))),
            first_month=("month", lambda x: sorted(set(x), key=month_key)[0]),
            example_label=("label", lambda s: s.dropna().iloc[0] if len(s.dropna()) else ""))
)

# Sort & Export CSV
rq1_top = agg.sort_values(["total_docs","months_covered"], ascending=[False, False])
rq1_path = os.path.join(OUT_DIR, "RQ1_top_topics.csv")
rq1_top.to_csv(rq1_path, index=False, encoding="utf-8-sig")
print("Saved:", rq1_path)

# Visualization: Top 20 Horizontal Bar Chart
topN = 20
fig = px.bar(
    rq1_top.head(topN),
    x="total_docs", y="theme_id",
    orientation="h",
    hover_data={"first_month": True, "months_covered": True, "example_label": True, "theme_id": False},
    title=f"Top {topN} most discussed themes (since {start_month})"
)
fig.update_layout(
    template="plotly_white",
    yaxis_title="theme_id",
    xaxis_title="Total posts",
    margin=dict(l=120, r=20, t=60, b=40)
)
fig.write_html(os.path.join(OUT_DIR, "RQ1_top_topics.html"))
fig.show()

import os, pandas as pd

base = OUT_BASE

print("df_time columns:", df_time.columns.tolist())
print("has topic_id:", "topic_id" in df_time.columns)

# 1) Verify that the ID matches.
def _read_ids(p):
    try:
        t = pd.read_csv(p)
        return set(t["theme_id"].unique().tolist()) if "theme_id" in t.columns else set()
    except Exception:
        return set()

ids_map = set(df_time["theme_id"].unique().tolist())
ids_idx = _read_ids(os.path.join(base, "_THEME_INDEX_", "theme_timeline.csv"))
ids_trk = _read_ids(os.path.join(base, "_THEME_TRACKING_", "theme_timeline.csv"))

print("IDs in month_topic_to_theme but not in INDEX timeline:", len(ids_map - ids_idx))
print("IDs in month_topic_to_theme but not in TRACKING timeline:", len(ids_map - ids_trk))

# 2) Check how many topics are missing the topic_id in the missing-tag topics section.
missing_ids = sorted(df_time.loc[df_time["theme_label"].eq(""), "theme_id"].unique())
if "topic_id" in df_time.columns:
    has_tid = df_time[df_time["theme_id"].isin(missing_ids)]["topic_id"].notna().groupby(df_time["theme_id"]).any()
    print("missing themes that DO have topic_id:", int(has_tid.sum()))
    print("missing themes that DO NOT have topic_id:", int((~has_tid).sum()))
else:
    print("No 'topic_id' column in df_time → Cannot populate using monthly top_words.")

def label_from_top_words(s: str, k=6):
    if not isinstance(s, str):
        return ""
    toks = [w.strip() for w in s.split(",") if w.strip()]
    DROP = {"hellip","amp","nbsp","quot"}
    out, seen = [], set()
    for w in toks:
        wl = w.lower()
        if wl in DROP or wl in seen:
            continue
        out.append(w); seen.add(wl)
        if len(out) >= k:
            break
    return " ; ".join(out)

def load_month_theme_table(out_base: str) -> pd.DataFrame:
    # 1) Read the month→theme mapping
    candidates = [
        os.path.join(out_base, "_THEME_INDEX_",    "month_topic_to_theme.csv"),
        os.path.join(out_base, "_THEME_TRACKING_", "month_topic_to_theme.csv"),
    ]
    for p in candidates:
        if os.path.exists(p):
            df = pd.read_csv(p)
            print("Loaded:", p)
            break
    else:
        raise FileNotFoundError("Cannot find month_topic_to_theme.csv (in _THEME_INDEX_ or _THEME_TRACKING_).")

    # Standard Listing
    colmap = {"Month":"month", "Theme":"theme_id", "Topic":"topic_id",
              "Count":"size", "docs":"size", "n_docs":"size"}
    df = df.rename(columns={c: colmap[c] for c in colmap if c in df.columns})

    need = {"month", "theme_id"}
    if not need.issubset(df.columns):
        raise KeyError(f"Required column is missing: {need - set(df.columns)}")

    if "size" not in df.columns:
        df["size"] = 1

    # Remove outliers
    df = df[df["theme_id"] != -1].copy()

    # Time Standardization and Screening
    df["month"] = df["month"].astype(str)
    df["year"]  = df["month"].str.slice(0, 4)
    if YEAR_MIN is not None:
        df = df[df["year"] >= str(YEAR_MIN)]
    if YEAR_MAX is not None:
        df = df[df["year"] <= str(YEAR_MAX)]

    # 2) First, retrieve existing tags from the timeline / theme_map.
    label_sources = [
        os.path.join(out_base, "_THEME_INDEX_",    "theme_timeline.csv"),
        os.path.join(out_base, "_THEME_TRACKING_", "theme_timeline.csv"),
        os.path.join(out_base, "_THEME_TRACKING_", "theme_map.csv"),
    ]
    label_map = {}
    for lp in label_sources:
        if os.path.exists(lp):
            t = pd.read_csv(lp)
            # Compatible column names: example_label/label
            if "theme_id" in t.columns and any(c in t.columns for c in ["example_label","label"]):
                labcol = "example_label" if "example_label" in t.columns else "label"
                t = t[["theme_id", labcol]].dropna()
                label_map.update(dict(zip(t["theme_id"], t[labcol].astype(str))))
                print("Label source used:", lp)

    # 3) For theme_id entries still lacking tags, generate tags using the top_words from the “month of first occurrence”.
    missing = sorted(set(df["theme_id"]) - set(label_map.keys()))
    if missing:
        # To locate the first occurrence of a topic_id within a given month, the month_topic_to_theme table must contain the corresponding topic_id (which has been standardized as per the naming convention above).
        if "topic_id" in df.columns:
            # First, find the first occurrence of each theme's month and its corresponding topic_id for that month.
            def _first_row(g):
                g = g.copy()
                g["__ord__"] = g["month"].map(lambda s: month_key(s))
                return g.sort_values("__ord__").iloc[0]
            firsts = df.groupby("theme_id", as_index=False).apply(_first_row).reset_index(drop=True)
            firsts = firsts[["theme_id","month","topic_id"]]

            for tid, m, local_tid in firsts.itertuples(index=False):
                if tid not in missing:
                    continue
                rep_path = os.path.join(out_base, m, "topic_representations.csv")
                if not os.path.exists(rep_path):
                    continue
                rep = pd.read_csv(rep_path)
                # Compatible Column Names
                if "Topic" in rep.columns and "topic_id" not in rep.columns:
                    rep = rep.rename(columns={"Topic":"topic_id"})
                if "top_words" not in rep.columns:
                    for col in ["Words","Representation"]:
                        if col in rep.columns:
                            rep = rep.rename(columns={col:"top_words"})
                            break
                row = rep[rep["topic_id"] == local_tid]
                if not row.empty and isinstance(row["top_words"].iloc[0], str):
                    label_map[tid] = label_from_top_words(row["top_words"].iloc[0], k=6)

    # Application Tags
    df["theme_label"] = df["theme_id"].map(label_map).fillna("")  # If still missing, the legend will display theme#id.
    return df

# ==== RQ2 / Cell 2: yearly aggregates ====

# Annual Total
year_sum = (df_time.groupby("year", as_index=False)["size"]
                  .sum().rename(columns={"size":"sum"}))

# Year × Theme Scale
year_theme = (df_time.groupby(["year","theme_id"], as_index=False)["size"]
                     .sum())

# Share = Size of the theme within the year / Total annual volume
year_theme = year_theme.merge(year_sum, on="year", how="left")
year_theme["share"] = (year_theme["size"] / year_theme["sum"]).fillna(0.0)

# Theme Tags
theme_labels = df_time[["theme_id","theme_label"]].drop_duplicates()
year_theme = year_theme.merge(theme_labels, on="theme_id", how="left")
year_theme["theme_label"] = year_theme["theme_label"].fillna("")

# Select Top-K (by cumulative scale across all years)
topk_ids = (year_theme.groupby("theme_id", as_index=False)["size"]
                     .sum().nlargest(TOPK, "size")["theme_id"].tolist())
yt_top = year_theme[year_theme["theme_id"].isin(topk_ids)].copy()

# Perspective (for drawing/exporting)
pivot_share = yt_top.pivot_table(index="year", columns="theme_id", values="share", fill_value=0.0)
pivot_size  = yt_top.pivot_table(index="year", columns="theme_id", values="size",  fill_value=0)

# Calculate the proportion of “others” (non-Top-K)
others_share = (1.0 - pivot_share.sum(axis=1)).clip(lower=0.0)
others_share.name = "others"

# Export
pivot_share.to_csv(os.path.join(OUT_DIR, "RQ2_pivot_share_topk.csv"), encoding="utf-8-sig")
pivot_size .to_csv(os.path.join(OUT_DIR, "RQ2_pivot_size_topk.csv"),  encoding="utf-8-sig")
print("Saved yearly tables to:", OUT_DIR)

# ==== RQ2 / Cell 3: visuals ====

def _label_for(tid: int) -> str:
    row = theme_labels[theme_labels["theme_id"] == tid]
    if not row.empty:
        lab = shorten_label(row["theme_label"].iloc[0] or "", maxlen=40)
        return f"theme#{tid}: {lab}" if lab else f"theme#{tid}"
    return f"theme#{tid}"

# Line Chart: Annual Share of Top-K Themes
plot_df = pivot_share.reset_index().melt(id_vars="year", var_name="theme_id", value_name="share")
plot_df["theme_id"] = plot_df["theme_id"].astype(int)
plot_df["label"] = plot_df["theme_id"].apply(_label_for)

fig1 = px.line(
    plot_df, x="year", y="share", color="label",
    title=f"Theme shares over years (Top {TOPK})", markers=True
)
fig1.update_layout(
    template="plotly_white",
    yaxis_title="Share of posts", xaxis_title="Year",
    yaxis_tickformat=".0%"
)
if MAKE_HTML:
    fig1.write_html(os.path.join(OUT_DIR, "RQ2_theme_shares_over_years.html"))
fig1.show()

# Stacked Area: Top-K + others
stack_df = pivot_share.copy()
stack_df["others"] = others_share
stack_df = stack_df.reset_index().melt(id_vars="year", var_name="key", value_name="share")

def label_or_others(k):
    if k == "others":
        return "others"
    return _label_for(int(k))

stack_df["label"] = stack_df["key"].apply(label_or_others)

fig2 = px.area(
    stack_df, x="year", y="share", color="label",
    title=f"Stacked area: Top {TOPK} themes + others"
)
fig2.update_layout(
    template="plotly_white",
    yaxis_title="Share of posts", xaxis_title="Year",
    yaxis_tickformat=".0%"
)
if MAKE_HTML:
    fig2.write_html(os.path.join(OUT_DIR, "RQ2_stacked_area_topk_plus_others.html"))
fig2.show()

# Heatmap: Year × Theme (share)
heat = pivot_share.copy()
heat.index.name = "year"
heat.columns = [_label_for(int(c)) for c in heat.columns]

fig3 = px.imshow(
    heat.T, aspect="auto", color_continuous_scale="Blues",
    title="Year × Theme (share heatmap)"
)
fig3.update_layout(
    template="plotly_white",
    xaxis_title="Year", yaxis_title="Theme",
    coloraxis_colorbar=dict(title="Share", tickformat=".0%")
)
if MAKE_HTML:
    fig3.write_html(os.path.join(OUT_DIR, "RQ2_heatmap_share.html"))
fig3.show()

# ==== RQ2 / Cell 4 (no-overlap YoY bars) ====

def yoy_table(pivot_share):
    df = pivot_share.copy()
    years = sorted(df.index.tolist())
    if len(years) < 2:
        return pd.DataFrame(columns=["theme_id","last_year","this_year","yoy_change"])
    t_this, t_last = years[-1], years[-2]
    y_this = df.loc[t_this]; y_last = df.loc[t_last]
    out = []
    for tid in df.columns:
        out.append({
            "theme_id": int(tid),
            "last_year": float(y_last[tid]),
            "this_year": float(y_this[tid]),
            "yoy_change": float(y_this[tid] - y_last[tid])
        })
    return pd.DataFrame(out).sort_values("yoy_change", ascending=False)

yoy = yoy_table(pivot_share)
yoy["label_full"]  = yoy["theme_id"].apply(lambda t: _label_for(t).replace("theme#",""))
yoy["label_short"] = yoy["label_full"].apply(shorten_label)

yoy_top    = yoy.nlargest(10, "yoy_change").iloc[::-1]
yoy_bottom = yoy.nsmallest(10, "yoy_change").iloc[::-1]

yoy.to_csv(os.path.join(OUT_DIR, "RQ2_yoy_change_topk.csv"),
           index=False, encoding="utf-8-sig")

def bar_no_overlap(df, title, filename):
    # Increase the line height and width to reduce stacking.
    h = 120 + 40 * len(df)

    fig = px.bar(
        df, x="yoy_change", y="label_short", orientation="h",
        title=title, text=df["yoy_change"].map(lambda v: f"{v:+.1%}")
    )


    x_min = float(df["yoy_change"].min())
    x_max = float(df["yoy_change"].max())
    span  = max(1e-6, x_max - x_min)
    pad   = max(0.02, 0.08 * span)
    fig.update_xaxes(range=[x_min - pad, x_max + pad])

    fig.update_traces(
        textposition="outside",
        cliponaxis=False,
        textfont_size=12,
        hovertemplate="<b>%{y}</b><br>YoY change=%{x:+.2%}<extra></extra>"
    )

    fig.update_layout(
        template="plotly_white",
        height=h,
        bargap=0.25,
        margin=dict(l=340, r=160, t=60, b=40),
        xaxis=dict(title="YoY change", tickformat="+.0%", automargin=True, zeroline=True),
        yaxis=dict(title="", automargin=True, tickfont=dict(size=12)),
        uniformtext_minsize=10,
        uniformtext_mode="hide"
    )

    if MAKE_HTML:
        fig.write_html(os.path.join(OUT_DIR, filename))
    fig.show()

bar_no_overlap(yoy_top,    "Top rising themes (YoY share change)",    "RQ2_rising_themes_yoy.html")

print("Updated figures saved to:", OUT_DIR)